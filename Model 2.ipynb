{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 2 \n",
    "Next, we're implementing a cleanup function and playing with dummy variables instead of our catgory replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, we just put together all our cleaning code into one function. This is mostly for better readability and repeatability There's no difference in methods yet. We're also calling the cleanup on our data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "\n",
    "def cleanup(data):\n",
    "    dow = {\n",
    "        \"Monday\" : 0,\n",
    "        \"Tuesday\" : 1,\n",
    "        \"Wednesday\" : 2,\n",
    "        \"Thursday\" : 3,\n",
    "        \"Friday\" : 4,\n",
    "        \"Saturday\" : 5,\n",
    "        \"Sunday\" : 6\n",
    "    }\n",
    "    data[\"DOW\"] = data.DayOfWeek.map(dow)\n",
    "    pds = {\n",
    "        \"SOUTHERN\" : 0,\n",
    "        \"MISSION\" : 1,\n",
    "        \"NORTHERN\" : 2,\n",
    "        \"BAYVIEW\" : 3,\n",
    "        \"CENTRAL\" : 4,\n",
    "        \"TERNDERLOIN\" : 5,\n",
    "        \"INGLESIDE\" : 6,\n",
    "        \"TARAVAL\" : 7,\n",
    "        \"PARK\" : 8,\n",
    "        \"RICHMOND\" : 9\n",
    "    }\n",
    "    data[\"pds\"] = data.PdDistrict.map(pds)\n",
    "    # for crimes without PD, use \"Other\" : 10\n",
    "    data[\"pds\"] = data[\"pds\"].fillna(10)\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    return data\n",
    "    \n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data = cleanup(data)\n",
    "                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummies we are going to attempt to use dummy variables to define new columns for more of our data. Using dummies instead of reassignment could increase precision (maybe?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Dates', u'Category', u'Descript', u'DayOfWeek', u'PdDistrict',\n",
       "       u'Resolution', u'Address', u'X', u'Y', u'Year', u'Month', u'Hour',\n",
       "       u'dow_Friday', u'dow_Monday', u'dow_Saturday', u'dow_Sunday',\n",
       "       u'dow_Thursday', u'dow_Tuesday', u'dow_Wednesday', u'pds_BAYVIEW',\n",
       "       u'pds_CENTRAL', u'pds_INGLESIDE', u'pds_MISSION', u'pds_NORTHERN',\n",
       "       u'pds_PARK', u'pds_RICHMOND', u'pds_SOUTHERN', u'pds_TARAVAL',\n",
       "       u'pds_TENDERLOIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanupDummies(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data =pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix=\"dow\")), axis=1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix=\"pds\")), axis=1)\n",
    "    return data\n",
    "\n",
    "data_dummies=pd.read_csv('train.csv')\n",
    "data_dummies= cleanupDummies(data_dummies)\n",
    "\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these cross validation trials with dummies and without dummies to look at how much it may have helped our score. We'll also be uploading a test csv to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1369992809902677"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "model = GaussianNB()\n",
    "cats = data.Category.values\n",
    "dataDrops = data.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "dummyDataDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model.fit(dataDrops.dropna(), cats)\n",
    "without_dummies_scores = cross_validation.cross_val_score(model, dataDrops, data[\"Category\"], cv = 3)\n",
    "\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "with_dummies_scores= cross_validation.cross_val_score(model, dummyDataDrops, data[\"Category\"], cv=3)\n",
    "\n",
    "with_dummies_scores.mean() - without_dummies_scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like using dummies drastically lowers our score, which means this could be a really good choice! We're going to go ahead and generate a test submission file for kaggle. Hopefully this isn't grossly overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('bernoulinb.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is not an improvement after submitting on kaggle! I'm going to keep experimenting with strat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
