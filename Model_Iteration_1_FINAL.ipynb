{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiki Chandra & Zoher Ghadyali\n",
    "\n",
    "Data Science Spring 2016\n",
    "\n",
    "CYOA Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ITERATION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iteration of the model, building off of the results of our data exploration, we break our data up into subsets of single words, sentences, and phrases. We then apply a variety of different techniques to determine the sentiment of these different categories. We use a bag of words or tfidf approach primarily for sentences. For single words, we look for whether or not they are positive or negative from other sources. For phrases, we average the sentiments obtained from the single words. In our second model, we break up the data further into sentences, long phrases, medium phrases, short phrases, and single words and follow a similar process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we set up our train and test data so we can implement a bag of words model while still separating our data into sentences, phrases, and single words. Our review to words function parses the sentences and the phrases and removes punctuation, stopwords, and makes all of the words lowercase. We create a placeholder in our test data for sentiment and we create our subset of sentences from the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    return( \" \".join( meaningful_words ))   \n",
    "\n",
    "train_df = pd.read_table('train.tsv')\n",
    "test_df = pd.read_table('test.tsv')\n",
    "\n",
    "test_df['Sentiment'] = -1 #a placeholder value, once we break up the test data into subsets, we can change this value\n",
    "train_sentences = train_df.groupby('SentenceId').first().reset_index()\n",
    "test_sentences = test_df.groupby('SentenceId').first().reset_index()\n",
    "train_sentences[\"Phrase\"] = train_sentences[\"Phrase\"].apply(review_to_words)\n",
    "test_sentences[\"Phrase\"] = test_sentences[\"Phrase\"].apply(review_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the tfidf vectorizer and a multinomial Naive Bayes model to fit the data. We have a very low accuracy from this model and one interesting thing to note is that this model predicts a majority of the sentences in the test dataset as 1s and 3s. There are not a lot of parameters to play around with in this model but this is very interesting behavior because we could get a baseline Kaggle score just by guessing 2 for every sentence, phrase, and single word, which we do later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.392437801424\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEZCAYAAADVBiHZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9T9pfN8g3ZbuAAAAAUlEQVSTQT6/FgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104a83d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 Percentage:  0.00211480362538\n",
      "Sentiment:  1 Percentage:  0.415709969789\n",
      "Sentiment:  2 Percentage:  0.0190332326284\n",
      "Sentiment:  3 Percentage:  0.548640483384\n",
      "Sentiment:  4 Percentage:  0.014501510574\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_sentences.Phrase)\n",
    "\n",
    "train_sentences_features = vectorizer.transform(train_sentences.Phrase)\n",
    "test_sentences_features = vectorizer.transform(test_sentences.Phrase)\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "scores = cross_validation.cross_val_score(clf, train_sentences_features, train_sentences[\"Sentiment\"], cv=10)\n",
    "print \"Accuracy\", scores.mean()\n",
    "\n",
    "clf = clf.fit(train_sentences_features, train_sentences[\"Sentiment\"])\n",
    "result = clf.predict(test_sentences_features)\n",
    "sentences = pd.DataFrame(data={\"PhraseId\":test_sentences[\"PhraseId\"], \"Sentiment\":result})\n",
    "\n",
    "plt.hist(sentences.Sentiment)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of the number of sentences for each predicted sentiment')\n",
    "plt.show()\n",
    "for i in range(0, 5):\n",
    "    print \"Sentiment: \", i, \"Percentage: \", len(sentences[sentences.Sentiment == i]) / float(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the single words in the data in the same way that we did during our data exploration. We then downloaded two .txt files from UNC that contain positive and negative words. In predicting the sentiment for the single words, we look to see if the given word is in either of these lists and assign it a 1 or a 3 if it is in the negative or positive list respectively. We do not assign 0s or 4s assuming that no single word has an exteme positive or negative sentiment without context. If the word is not in either list, we assign it a 2.\n",
    "\n",
    "We plot a histogram of the sentiment for the single words and we find that 83% of the single words have a sentiment of 2 which makes sense given that the positive and negative word text files only contain 3000 words each. Most words are probably not in either list and so given a sentiment of 2. We do find that 16% of single words are contained in one of these lists and, unsurprisingly, there are 0 words with a sentiment of 0 or 4.\n",
    "\n",
    "Finally, we merge the sentiments that our tfidf model predicted for the sentences with the sentiments predicted for the single words. This lets us now find all remaning phrases that we haven't yet predicted the sentiment for and, once we do predict their sentiment, we can merge that dataframe into this merged one to have predictions for all of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEZCAYAAADG0WEtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1049af490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0 Percentage:  0.0\n",
      "Sentiment:  1 Percentage:  0.0913720790893\n",
      "Sentiment:  2 Percentage:  0.830037946874\n",
      "Sentiment:  3 Percentage:  0.0785899740363\n",
      "Sentiment:  4 Percentage:  0.0\n"
     ]
    }
   ],
   "source": [
    "negWords = open(\"negative.txt\").read().split(\"\\n\")\n",
    "posWords = open(\"positive.txt\").read().split(\"\\n\")\n",
    "\n",
    "test_df[\"containsSpaces\"] = test_df[\"Phrase\"].apply(lambda x: ' ' in x)\n",
    "test_sw = test_df[test_df.containsSpaces == False]\n",
    "\n",
    "def get_sentiment_single_word(word):\n",
    "    \"\"\"Takes in as input a single word, checks if the word is in two text files we downloaded of negative and positive\n",
    "    words, and returns the corresponding value. We use 1 and 3 instead of 0 and 4 because we assume no single word can\n",
    "    have an extremely negative or positive sentiment.\"\"\"\n",
    "    if word in negWords:\n",
    "        return 1\n",
    "    elif word in posWords:\n",
    "        return 3\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "test_sw['Sentiment'] = test_sw['Phrase'].apply(get_sentiment_single_word)\n",
    "\n",
    "sw = test_sw[['PhraseId', 'Sentiment']]\n",
    "\n",
    "plt.hist(sw.Sentiment)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of the number of single words for each predicted sentiment')\n",
    "plt.show()\n",
    "for i in range(0, 5):\n",
    "    print \"Sentiment: \", i, \"Percentage: \", len(sw[sw.Sentiment == i]) / float(len(sw))\n",
    "\n",
    "merged1 = pd.merge(sentences, sw, how='outer') #outer gets all rows from both dataframes which is what we want to\n",
    "                                               #produce all the rows we need in the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get all the remaning phrases that we have not yet predicted the sentiment for in the test data and we clean them. We create a dictionary using the dataframe we created of the single words to essentially use as a lookup table. We loop through the words in each phrase, get their sentiment from the dictionary or, if they aren't in the dictionary add a 2, and then average the list. If the list is empty, we return a 2.\n",
    "\n",
    "We then merge the remaining phrases with our previous merged data frame that contained our sentences and single words. We print out the length of the data frame after this merge to ensure that we have calculated the sentiment for all of the rows in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292\n"
     ]
    }
   ],
   "source": [
    "remainingPhrases = test_df[~test_df['PhraseId'].isin(merged1['PhraseId'])].reset_index()\n",
    "remainingPhrases[\"Phrase\"] = remainingPhrases[\"Phrase\"].apply(review_to_words)\n",
    "\n",
    "sw_dict = test_sw.set_index('Phrase')['Sentiment'].to_dict()\n",
    "\n",
    "def get_sentiment_phrase(phrase):\n",
    "    phrase_sentiment = []\n",
    "    for word in phrase.split():\n",
    "        if word in sw_dict:\n",
    "            phrase_sentiment.append(sw_dict[word])\n",
    "        else:\n",
    "            phrase_sentiment.append(2)\n",
    "    if len(phrase_sentiment):\n",
    "        return sum(phrase_sentiment)/len(phrase_sentiment)\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "remainingPhrases['Sentiment'] = remainingPhrases['Phrase'].apply(get_sentiment_phrase)\n",
    "phrases = remainingPhrases[['PhraseId', 'Sentiment']]\n",
    "merged2 = pd.merge(merged1, phrases, how='outer')\n",
    "print len(merged2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have verified that we have predicted the sentiment for all of the phrases, sentences, and single words in the test data, we sort by PhraseId, ensure that the PhraseId and Sentiments are stored as integers and not as strings, and output the result as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = merged2.sort_values(by='PhraseId')\n",
    "final['PhraseId'] = final['PhraseId'].apply(lambda x: int(x))\n",
    "final['Sentiment'] = final['Sentiment'].apply(lambda x: int(x))\n",
    "final.to_csv( \"ZG_rotten_tomatoes_model1.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This csv file got a Kaggle score of 0.52572. This method is somewhat difficult to interpret because we cannot gauge the accuracy of our models separately. We do not know if our bag of words model is very accurate or if our functions for single words and phrases is good. To see how this model compares to a baseline, we decided to just guess 2 for every row in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline = merged2.sort_values(by='PhraseId')\n",
    "baseline['PhraseId'] = final['PhraseId'].apply(lambda x: int(x))\n",
    "baseline['Sentiment'] = final['Sentiment'].apply(lambda x: 2)\n",
    "baseline.to_csv( \"ZG_rotten_tomatoes_baseline.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By guessing 2 for every row in the test data, we get a Kaggle score of 0.51789. So we see that our model for sentences, phrases, and single words is only providing a 0.00783% improvement than just guessing 2. We try a slightly different approach below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tried using the bag of words on separate groups of sentences, long phrases, phrases, shorter phrases, and single words. To do this, we created a length of words in phrase column and binned the rows into the group that corresponded with the phrase length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 26, 13, 3, 1, 0]\n",
      "4193 20141 68215 46979 16531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiki/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Length</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>Sentences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the gander , some of which occasional...</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>for the gander , some of which occasionally am...</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which occasionally amuses but none of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>occasionally amuses but none of which amounts ...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>amuses but none of which amounts to much of a ...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>but none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>of which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>to much of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the goose</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>much of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>for the goose</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander ,</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>of a story</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156025</th>\n",
       "      <td>156026</td>\n",
       "      <td>8542</td>\n",
       "      <td>to parody</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156031</th>\n",
       "      <td>156032</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall is to substitute plot fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156033</th>\n",
       "      <td>156034</td>\n",
       "      <td>8543</td>\n",
       "      <td>is to substitute plot for personality .</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156034</th>\n",
       "      <td>156035</td>\n",
       "      <td>8543</td>\n",
       "      <td>is to substitute plot for personality</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156035</th>\n",
       "      <td>156036</td>\n",
       "      <td>8543</td>\n",
       "      <td>to substitute plot for personality</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156032</th>\n",
       "      <td>156033</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156036</th>\n",
       "      <td>156037</td>\n",
       "      <td>8543</td>\n",
       "      <td>substitute plot for personality</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156037</th>\n",
       "      <td>156038</td>\n",
       "      <td>8543</td>\n",
       "      <td>substitute plot</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156038</th>\n",
       "      <td>156039</td>\n",
       "      <td>8543</td>\n",
       "      <td>for personality</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156039</th>\n",
       "      <td>156040</td>\n",
       "      <td>8544</td>\n",
       "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156040</th>\n",
       "      <td>156041</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156041</th>\n",
       "      <td>156042</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric , with Herrmann quietly ...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156044</th>\n",
       "      <td>156045</td>\n",
       "      <td>8544</td>\n",
       "      <td>with Herrmann quietly suggesting the sadness a...</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>LongPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156045</th>\n",
       "      <td>156046</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann quietly suggesting the sadness and ob...</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156047</th>\n",
       "      <td>156048</td>\n",
       "      <td>8544</td>\n",
       "      <td>quietly suggesting the sadness and obsession b...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156048</th>\n",
       "      <td>156049</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession beneath H...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156053</th>\n",
       "      <td>156054</td>\n",
       "      <td>8544</td>\n",
       "      <td>beneath Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156049</th>\n",
       "      <td>156050</td>\n",
       "      <td>8544</td>\n",
       "      <td>suggesting the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156054</th>\n",
       "      <td>156055</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's forced avuncular chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156042</th>\n",
       "      <td>156043</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric ,</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156050</th>\n",
       "      <td>156051</td>\n",
       "      <td>8544</td>\n",
       "      <td>the sadness and obsession</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156043</th>\n",
       "      <td>156044</td>\n",
       "      <td>8544</td>\n",
       "      <td>is darkly atmospheric</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156051</th>\n",
       "      <td>156052</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and obsession</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156052</th>\n",
       "      <td>156053</td>\n",
       "      <td>8544</td>\n",
       "      <td>sadness and</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>SmallPhrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156046</th>\n",
       "      <td>156047</td>\n",
       "      <td>8544</td>\n",
       "      <td>Herrmann</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>SingleWord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>SingleWord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>SingleWord</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "27            28           1   \n",
       "28            29           1   \n",
       "31            32           1   \n",
       "32            33           1   \n",
       "33            34           1   \n",
       "38            39           1   \n",
       "1              2           1   \n",
       "5              6           1   \n",
       "43            44           1   \n",
       "7              8           1   \n",
       "45            46           1   \n",
       "9             10           1   \n",
       "47            48           1   \n",
       "49            50           1   \n",
       "51            52           1   \n",
       "15            16           1   \n",
       "52            53           1   \n",
       "17            18           1   \n",
       "53            54           1   \n",
       "19            20           1   \n",
       "55            56           1   \n",
       "21            22           1   \n",
       "57            58           1   \n",
       "10            11           1   \n",
       "23            24           1   \n",
       "34            35           1   \n",
       "39            40           1   \n",
       "59            60           1   \n",
       "2              3           1   \n",
       "...          ...         ...   \n",
       "156025    156026        8542   \n",
       "156031    156032        8543   \n",
       "156033    156034        8543   \n",
       "156034    156035        8543   \n",
       "156035    156036        8543   \n",
       "156032    156033        8543   \n",
       "156036    156037        8543   \n",
       "156037    156038        8543   \n",
       "156038    156039        8543   \n",
       "156039    156040        8544   \n",
       "156040    156041        8544   \n",
       "156041    156042        8544   \n",
       "156044    156045        8544   \n",
       "156045    156046        8544   \n",
       "156047    156048        8544   \n",
       "156048    156049        8544   \n",
       "156053    156054        8544   \n",
       "156049    156050        8544   \n",
       "156054    156055        8544   \n",
       "156042    156043        8544   \n",
       "156050    156051        8544   \n",
       "156043    156044        8544   \n",
       "156051    156052        8544   \n",
       "156056    156057        8544   \n",
       "156052    156053        8544   \n",
       "156055    156056        8544   \n",
       "156057    156058        8544   \n",
       "156046    156047        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  Length  \\\n",
       "0       A series of escapades demonstrating the adage ...          1      37   \n",
       "27      is also good for the gander , some of which oc...          2      23   \n",
       "28      is also good for the gander , some of which oc...          2      22   \n",
       "31      good for the gander , some of which occasional...          2      20   \n",
       "32      for the gander , some of which occasionally am...          2      19   \n",
       "33      the gander , some of which occasionally amuses...          1      18   \n",
       "38      some of which occasionally amuses but none of ...          2      15   \n",
       "1       A series of escapades demonstrating the adage ...          2      14   \n",
       "5       of escapades demonstrating the adage that what...          2      12   \n",
       "43      occasionally amuses but none of which amounts ...          2      12   \n",
       "7       escapades demonstrating the adage that what is...          2      11   \n",
       "45      amuses but none of which amounts to much of a ...          2      11   \n",
       "9       demonstrating the adage that what is good for ...          2      10   \n",
       "47           but none of which amounts to much of a story          1      10   \n",
       "49               none of which amounts to much of a story          1       9   \n",
       "51                    of which amounts to much of a story          2       8   \n",
       "15                        that what is good for the goose          2       7   \n",
       "52                       which amounts to much of a story          2       7   \n",
       "17                             what is good for the goose          2       6   \n",
       "53                             amounts to much of a story          2       6   \n",
       "19                                  is good for the goose          2       5   \n",
       "55                                     to much of a story          2       5   \n",
       "21                                     good for the goose          3       4   \n",
       "57                                        much of a story          2       4   \n",
       "10                                demonstrating the adage          2       3   \n",
       "23                                          for the goose          2       3   \n",
       "34                                           the gander ,          2       3   \n",
       "39                                          some of which          2       3   \n",
       "59                                             of a story          2       3   \n",
       "2                                                A series          2       2   \n",
       "...                                                   ...        ...     ...   \n",
       "156025                                          to parody          2       2   \n",
       "156031  The movie 's downfall is to substitute plot fo...          1      11   \n",
       "156033            is to substitute plot for personality .          1       7   \n",
       "156034              is to substitute plot for personality          1       6   \n",
       "156035                 to substitute plot for personality          2       5   \n",
       "156032                              The movie 's downfall          1       4   \n",
       "156036                    substitute plot for personality          1       4   \n",
       "156037                                    substitute plot          2       2   \n",
       "156038                                    for personality          2       2   \n",
       "156039  The film is darkly atmospheric , with Herrmann...          2      21   \n",
       "156040  is darkly atmospheric , with Herrmann quietly ...          2      19   \n",
       "156041  is darkly atmospheric , with Herrmann quietly ...          2      18   \n",
       "156044  with Herrmann quietly suggesting the sadness a...          2      14   \n",
       "156045  Herrmann quietly suggesting the sadness and ob...          2      13   \n",
       "156047  quietly suggesting the sadness and obsession b...          1      12   \n",
       "156048  suggesting the sadness and obsession beneath H...          2      11   \n",
       "156053        beneath Hearst 's forced avuncular chortles          2       6   \n",
       "156049               suggesting the sadness and obsession          2       5   \n",
       "156054                Hearst 's forced avuncular chortles          2       5   \n",
       "156042                            is darkly atmospheric ,          2       4   \n",
       "156050                          the sadness and obsession          2       4   \n",
       "156043                              is darkly atmospheric          3       3   \n",
       "156051                              sadness and obsession          1       3   \n",
       "156056                          forced avuncular chortles          1       3   \n",
       "156052                                        sadness and          1       2   \n",
       "156055                                          Hearst 's          2       2   \n",
       "156057                                 avuncular chortles          3       2   \n",
       "156046                                           Herrmann          2       1   \n",
       "156058                                          avuncular          2       1   \n",
       "156059                                           chortles          2       1   \n",
       "\n",
       "         categories  \n",
       "0         Sentences  \n",
       "27       LongPhrase  \n",
       "28       LongPhrase  \n",
       "31       LongPhrase  \n",
       "32       LongPhrase  \n",
       "33       LongPhrase  \n",
       "38       LongPhrase  \n",
       "1        LongPhrase  \n",
       "5            Phrase  \n",
       "43           Phrase  \n",
       "7            Phrase  \n",
       "45           Phrase  \n",
       "9            Phrase  \n",
       "47           Phrase  \n",
       "49           Phrase  \n",
       "51           Phrase  \n",
       "15           Phrase  \n",
       "52           Phrase  \n",
       "17           Phrase  \n",
       "53           Phrase  \n",
       "19           Phrase  \n",
       "55           Phrase  \n",
       "21           Phrase  \n",
       "57           Phrase  \n",
       "10      SmallPhrase  \n",
       "23      SmallPhrase  \n",
       "34      SmallPhrase  \n",
       "39      SmallPhrase  \n",
       "59      SmallPhrase  \n",
       "2       SmallPhrase  \n",
       "...             ...  \n",
       "156025  SmallPhrase  \n",
       "156031       Phrase  \n",
       "156033       Phrase  \n",
       "156034       Phrase  \n",
       "156035       Phrase  \n",
       "156032       Phrase  \n",
       "156036       Phrase  \n",
       "156037  SmallPhrase  \n",
       "156038  SmallPhrase  \n",
       "156039   LongPhrase  \n",
       "156040   LongPhrase  \n",
       "156041   LongPhrase  \n",
       "156044   LongPhrase  \n",
       "156045       Phrase  \n",
       "156047       Phrase  \n",
       "156048       Phrase  \n",
       "156053       Phrase  \n",
       "156049       Phrase  \n",
       "156054       Phrase  \n",
       "156042       Phrase  \n",
       "156050       Phrase  \n",
       "156043  SmallPhrase  \n",
       "156051  SmallPhrase  \n",
       "156056  SmallPhrase  \n",
       "156052  SmallPhrase  \n",
       "156055  SmallPhrase  \n",
       "156057  SmallPhrase  \n",
       "156046   SingleWord  \n",
       "156058   SingleWord  \n",
       "156059   SingleWord  \n",
       "\n",
       "[156060 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_table('train.tsv')\n",
    "train[\"Length\"] = train['Phrase'].apply(lambda x: len(x.split()))\n",
    "ordered = train.sort(['SentenceId', 'Length'], ascending=[1, 0])\n",
    "train_sentences = train.groupby('SentenceId').first().reset_index()\n",
    "\n",
    "\n",
    "max_length = max(ordered['Length'])\n",
    "bins = [0, 1, max_length/15, max_length/4, max_length/2, max_length]\n",
    "group_names = ['SingleWord', 'SmallPhrase', 'Phrase', 'LongPhrase', 'Sentences']\n",
    "categories = pd.cut(ordered['Length'], bins, labels=group_names)\n",
    "ordered['categories'] = pd.cut(ordered['Length'], bins, labels=group_names)\n",
    "\n",
    "\n",
    "sentences = ordered[ordered['categories'] == 'Sentences'].reset_index()\n",
    "longphrase = ordered[ordered['categories'] == 'LongPhrase'].reset_index()\n",
    "phrase = ordered[ordered['categories'] == 'Phrase'].reset_index()\n",
    "smallphrase = ordered[ordered['categories'] == 'SmallPhrase'].reset_index()\n",
    "singleword = ordered[ordered['categories'] == 'SingleWord'].reset_index()\n",
    "print bins[::-1]\n",
    "print len(sentences), len(longphrase), len(phrase), len(smallphrase), len(singleword)\n",
    "ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a basic cleaning function to parse the phrase and remove words that aren't considered to be meaningful words. These words are typically called stopwords, containing words such as 'is' and 'the'. We then iterate through each data frame and clean the phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_reviews_sent = sentences[\"Phrase\"].size\n",
    "clean_sent_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_sent):\n",
    "    clean_sent_reviews.append( review_to_words(sentences[\"Phrase\"][i]))\n",
    "    \n",
    "num_reviews_long = longphrase[\"Phrase\"].size\n",
    "clean_long_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_long):\n",
    "    clean_long_reviews.append( review_to_words(longphrase[\"Phrase\"][i]))\n",
    "    \n",
    "num_reviews_p = phrase[\"Phrase\"].size\n",
    "clean_p_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_p):\n",
    "    clean_p_reviews.append( review_to_words(phrase[\"Phrase\"][i]))\n",
    "    \n",
    "num_reviews_small = smallphrase[\"Phrase\"].size\n",
    "clean_small_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_small):\n",
    "    clean_small_reviews.append( review_to_words(smallphrase[\"Phrase\"][i]))\n",
    "\n",
    "num_reviews_single = singleword[\"Phrase\"].size\n",
    "clean_single_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_single):\n",
    "    clean_single_reviews.append( review_to_words(singleword[\"Phrase\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used count vectorizer to create a count of the word tokens as vectors, representing the amount of times they appear in phrase. We played around with the max_features values to see how that would increase the accuracy - it really only made a difference of about ~0.004 at most, which is good but not very significant. Then, we applied our models to each data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 4000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "sent_data_features = vectorizer.fit_transform(clean_sent_reviews)\n",
    "sent_data_features = sent_data_features.toarray()\n",
    "\n",
    "long_data_features = vectorizer.fit_transform(clean_long_reviews)\n",
    "long_data_features = long_data_features.toarray()\n",
    "\n",
    "p_data_features = vectorizer.fit_transform(clean_p_reviews)\n",
    "p_data_features = p_data_features.toarray()\n",
    "\n",
    "small_data_features = vectorizer.fit_transform(clean_small_reviews)\n",
    "small_data_features = small_data_features.toarray()\n",
    "\n",
    "single_data_features = vectorizer.fit_transform(clean_single_reviews)\n",
    "single_data_features = single_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print sent_data_features.shape, long_data_features.shape, p_data_features.shape, small_data_features.shape, single_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes: (4193, 4000) (20141, 4000) (68215, 4000) (46979, 4000) (16531, 4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the vectorizer ran as intended, we then printed out some values in the vocabulary created from running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(sent_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 aaa <br>\n",
    "37 able <br>\n",
    "5 absorbed <br>\n",
    "8 accident <br>\n",
    "7 act <br>\n",
    "10 acted <br>\n",
    "5 acting <br>\n",
    "10 action <br>\n",
    "10 actor <br>\n",
    "5 actress <br>\n",
    "5 adolescent <br>\n",
    "8 adrenaline <br>\n",
    "5 adult <br>\n",
    "6 adults <br>\n",
    "13 adventure <br>\n",
    "6 advised <br>\n",
    "9 affectation <br>\n",
    "5 affirming <br>\n",
    "8 african <br>\n",
    "5 age <br>\n",
    "5 aged <br>\n",
    "12 agency <br>\n",
    "17 agenda <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use a random forest classifier to see its accuracy compared to the multinomial model that we previously ran. Here we train the model with the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest_sent = RandomForestClassifier(n_estimators = 50)\n",
    "forest_long = RandomForestClassifier(n_estimators = 40) \n",
    "forest_p = RandomForestClassifier(n_estimators = 30)\n",
    "forest_small = RandomForestClassifier(n_estimators = 20)\n",
    "forest_single = RandomForestClassifier(n_estimators = 20) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "# This may take a few minutes to run\n",
    "forest_sent = forest_sent.fit(sent_data_features, sentences[\"Sentiment\"])\n",
    "forest_long = forest_long.fit(long_data_features, longphrase[\"Sentiment\"])\n",
    "forest_p = forest_p.fit(p_data_features, phrase[\"Sentiment\"])\n",
    "forest_small = forest_small.fit(small_data_features, smallphrase[\"Sentiment\"])\n",
    "forest_single = forest_single.fit(single_data_features, singleword[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, we ran the above code on the test data, creating separate data frames and cleaning the data. Then, we apply the random forest models to each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered = test.sort(['SentenceId', 'Length'], ascending=[1, 0])\n",
    "\n",
    "max_length = max(ordered['Length'])\n",
    "bins = [0, 1, max_length/15, max_length/4, max_length/2, max_length]\n",
    "group_names = ['SingleWord', 'SmallPhrase', 'Phrase', 'LongPhrase', 'Sentences']\n",
    "categories = pd.cut(ordered['Length'], bins, labels=group_names)\n",
    "ordered['categories'] = pd.cut(ordered['Length'], bins, labels=group_names)\n",
    "\n",
    "\n",
    "sentences = ordered[ordered['categories'] == 'Sentences'].reset_index()\n",
    "longphrase = ordered[ordered['categories'] == 'LongPhrase'].reset_index()\n",
    "phrase = ordered[ordered['categories'] == 'Phrase'].reset_index()\n",
    "smallphrase = ordered[ordered['categories'] == 'SmallPhrase'].reset_index()\n",
    "singleword = ordered[ordered['categories'] == 'SingleWord'].reset_index()\n",
    "\n",
    "\n",
    "num_reviews_sent = sentences[\"Phrase\"].size\n",
    "clean_sent_reviews = []\n",
    "\n",
    "print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "for i in xrange(0, num_reviews_sent):\n",
    "    clean_sent_reviews.append( review_to_words(sentences[\"Phrase\"][i]))\n",
    "    if( (i+1) % 10000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, len(ordered))\n",
    "    \n",
    "num_reviews_long = longphrase[\"Phrase\"].size\n",
    "clean_long_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_long):\n",
    "    clean_long_reviews.append( review_to_words(longphrase[\"Phrase\"][i]))\n",
    "    if( (i+1) % 10000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, len(ordered))\n",
    "        \n",
    "num_reviews_p = phrase[\"Phrase\"].size\n",
    "clean_p_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_p):\n",
    "    clean_p_reviews.append( review_to_words(phrase[\"Phrase\"][i]))\n",
    "    if( (i+1) % 10000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, len(ordered))\n",
    "    \n",
    "num_reviews_small = smallphrase[\"Phrase\"].size\n",
    "clean_small_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_small):\n",
    "    clean_small_reviews.append( review_to_words(smallphrase[\"Phrase\"][i]))\n",
    "    if( (i+1) % 10000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, len(ordered))\n",
    "\n",
    "num_reviews_single = singleword[\"Phrase\"].size\n",
    "clean_single_reviews = []\n",
    "\n",
    "for i in xrange(0, num_reviews_single):\n",
    "    clean_single_reviews.append( review_to_words(singleword[\"Phrase\"][i]))\n",
    "    if( (i+1) % 10000 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, len(ordered))\n",
    "\n",
    "print \"Applying tree to sentences...\\n\"        \n",
    "sent_data_features = vectorizer.fit_transform(clean_sent_reviews)\n",
    "sent_data_features = sent_data_features.toarray()\n",
    "result_sent = forest_sent.predict(sent_data_features)\n",
    "\n",
    "print \"Applying tree to long phrases...\\n\" \n",
    "long_data_features = vectorizer.fit_transform(clean_long_reviews)\n",
    "long_data_features = long_data_features.toarray()\n",
    "result_long = forest_long.predict(long_data_features)\n",
    "\n",
    "print \"Applying tree to mid phrases...\\n\" \n",
    "p_data_features = vectorizer.fit_transform(clean_p_reviews)\n",
    "p_data_features = p_data_features.toarray()\n",
    "result_p = forest_p.predict(p_data_features)\n",
    "\n",
    "print \"Applying tree to small phrases...\\n\" \n",
    "small_data_features = vectorizer.fit_transform(clean_small_reviews)\n",
    "small_data_features = small_data_features.toarray()\n",
    "result_small = forest_small.predict(small_data_features)\n",
    "\n",
    "print \"Applying tree to single words...\\n\" \n",
    "single_data_features = vectorizer.fit_transform(clean_single_reviews)\n",
    "single_data_features = single_data_features.toarray()\n",
    "result_single = forest_single.predict(single_data_features)\n",
    "\n",
    "#print result_sent, result_long, result_small, result_single\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "#output = pd.DataFrame(data={\"PhraseId\":test_sentences[\"PhraseId\"], \"Sentiment\":result})\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "#output.to_csv( \"rotten_tomatoes_model.csv\", index=False, quoting=3 )\n",
    "print \"done predicting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and parsing the test set movie reviews... <br>\n",
    "\n",
    "Review 10000 of 66292<br>\n",
    "\n",
    "Review 20000 of 66292<br>\n",
    "\n",
    "Review 10000 of 66292<br>\n",
    "\n",
    "Review 10000 of 66292<br>\n",
    "\n",
    "Applying tree to sentences...<br>\n",
    "\n",
    "Applying tree to long phrases...<br>\n",
    "\n",
    "Applying tree to mid phrases...<br>\n",
    "\n",
    "Applying tree to small phrases...<br>\n",
    "\n",
    "Applying tree to single words...<br>\n",
    "\n",
    "done predicting<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finished applying the models, we set the test data 'Sentiment' column to the values generated through the model. We also printed out the length to verify that we didn't lose any rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences['Sentiment'] = result_sent.tolist()\n",
    "longphrase['Sentiment'] = result_long.tolist()\n",
    "phrase['Sentiment'] = result_p.tolist()\n",
    "smallphrase['Sentiment'] = result_small.tolist()\n",
    "singleword['Sentiment'] = result_single.tolist()\n",
    "print len(sentences)+len(longphrase)+len(phrase)+len(smallphrase)+len(singleword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"PhraseId\":df3[\"PhraseId\"], \"Sentiment\":df3['Sentiment']})\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"rotten_tomatoes_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle score: 0.5181 <br>\n",
    "Running this model gave us a slight improvement on the multinomial model. However, much of our structure and process was determined through looking at tutorials so we could get a chance to learn in a structured way. Now that we have a better sense of what can impact the overall accuracy, and what different directions we can go in, we will try new models that can be significant improvements on these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried a lot of very different things in this first iteration of the model but because we do not have access to the actual sentiments of the test data, we found it really hard to find the problems with this iteration of the model since we couldn't tell how accurate our sentences model, single words model, and phrases model were doing. Even if we used a train-test split on the train data, in some of the subsets, especially the sentences and single words, there are so few to train and test on that we didn't feel like we could accurately predict how the model would do on the test data. \n",
    "\n",
    "We tried to make the model less opaque in our second iteration, which is contained in Model_Iteration_2_FINAL.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
