{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import crime\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OAK ST / LAGUNA ST' 'VANNESS AV / GREENWICH ST'\n",
      " '1500 Block of LOMBARD ST' ..., '300 Block of JOHN F KENNEDY DR'\n",
      " 'FOLSOM ST / ZENO PL' '1000 Block of 22ND AV']\n"
     ]
    }
   ],
   "source": [
    "train = crime.load_cleaned_train()\n",
    "test = crime.load_cleaned_test()\n",
    "\n",
    "train.columns\n",
    "# print train.info()\n",
    "# print test.info()\n",
    "\n",
    "print train.Address.unique()\n",
    "\n",
    "at_corner = []\n",
    "for address in train.Address:\n",
    "    at_corner.append('/' in address)\n",
    "train['CornerCrime'] = at_corner\n",
    "\n",
    "at_corner = []\n",
    "for address in test.Address:\n",
    "    at_corner.append('/' in address)\n",
    "test['CornerCrime'] = at_corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is cleaned as described in `crime.py`.  In short, Year, Month, Day, Hour, and Minute columns are created, DayOfWeek, PdDistrict, and Category are encoded as integers, and invalid X and Y values are set to the median for that crime's PdDistrict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split Train Data for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "predictors = ['X', 'Y', 'Hour','CornerCrime', 'PdD']\n",
    "# predictors = ['X', 'Y']\n",
    "X = train[predictors]\n",
    "y = train.CategoryNumber\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `stratify` parameter of `train_test_split` requires scikit-learn-0.17, but ensures that the proportion of categories is maintained in the split.  The biggest thing that this does is make it so that we always get at least one crime from each category in the training set.  k-NN can only predict based on what it has seen before, so it is crucial that we train the model with all possible categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a quick run through of various models. Those models which we submit to kaggle are noted below. To see a more comprehensive outline of models and how they worked, please see the models ipython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.21681892914\n"
     ]
    }
   ],
   "source": [
    "alg = KNeighborsClassifier(n_neighbors=50, n_jobs=-1)\n",
    "alg.fit(X_train, y_train)\n",
    "\n",
    "p = alg.predict_proba(X_test)\n",
    "print crime.logloss(y_test, p)\n",
    "\n",
    "#let's make a submission\n",
    "crime.create_submission(alg, X, y, test, predictors, 'v1_Knn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6668366232023684"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = LogisticRegression()\n",
    "alg.fit(X_train,y_train)\n",
    "\n",
    "p = alg.predict_proba(X_test)\n",
    "crime.logloss(y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5539968194182818"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = tree.DecisionTreeClassifier(max_depth=6)\n",
    "alg.fit(X_train,y_train)\n",
    "\n",
    "p = alg.predict_proba(X_test)\n",
    "crime.logloss(y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6956344784492186"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "alg.fit(X_train,y_train)\n",
    "\n",
    "p = alg.predict_proba(X_test)\n",
    "crime.logloss(y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.68898640645883"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "gnb = BernoulliNB()\n",
    "y_pred = gnb.fit(X_test, y_test).predict_proba(X_train)\n",
    "# print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "#        % (X_test.shape[0],(y_train != y_pred).sum()))\n",
    "\n",
    "crime.logloss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41934871946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators = 25, max_depth = 10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "p = clf.predict_proba(X_test)\n",
    "print crime.logloss(y_test,p)\n",
    "\n",
    "#let's make a submission\n",
    "crime.create_submission(clf, X, y, test, predictors, 'v1_RFC_CC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.1832521143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "alg = SGDClassifier(loss='log', penalty='l2', n_iter=10)\n",
    "alg.fit(X_train, y_train)\n",
    "\n",
    "p = alg.predict_proba(X_test)\n",
    "print crime.logloss(y_test,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Submission Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier performed very well on our split testing data, so we put this into kaggle. With simple parameters, we were able to achieve a score on our test data of 2.57091. With tuning, we achieved a score of 2.54702, and then simply using location information a score of 2.45693 with a number of iterations of 25. With predictors of location, police district, and hour this got to 2.44754 (the split test only had a score of 2.44835, so clearly you can't completely judge the \"goodness\" of the model until you run it on the test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
