{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emily Wang and Filippos Lymperopoulos | Data Science 2016 | CYOA: sfcrime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 15, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process\n",
    "\n",
    "* Import libraries and training data\n",
    "* Feature engineering / preprocessing: \n",
    "    * Make \"useful\" combinations of features to give our model; \n",
    "    * Also encode categorical things in an intelligent way; \n",
    "    * Can choose to only use a subset of features if desired\n",
    "* Partition your data (cross-validation kfolds, etc)\n",
    "* Model fit\n",
    "* Make some predictions\n",
    "* Compute the logloss score\n",
    "* Reflect; iterate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's import some useful libraries and import the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                  Descript  DayOfWeek  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS            WARRANT ARREST  Wednesday   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "\n",
       "  PdDistrict      Resolution                    Address           X          Y  \n",
       "0   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "1   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "2   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST -122.424363  37.800414  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "# Convert the Dates column of our provided data from string to datetime format.\n",
    "train = pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test = pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "# Print the first 3 rows of the dataframe.\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Preprocessing\n",
    "\n",
    "Make a class that will:\n",
    "* Extract the time features we want to use for the model (e.g. year, season, month, day, etc.)\n",
    "* Encode categorical variables in a meaningful way: contains a preprocessor that can both transform and inverse_transform the categorical variables\n",
    "* Return a transformed dataframe to be given to the model\n",
    "* Maybe: allow for some flexibility with what is in the transformed dataframe (to iterate quickly) (e.g. choosing how many time features you want in this experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SFP = SFCrime Preprocessor\n",
    "class SFP():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.Y_encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # Prepare inputs\n",
    "    def prep_district(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.PdDistrict)\n",
    "    \n",
    "    def prep_hour(self):\n",
    "        # a continuous value from 0 to 23\n",
    "        return self.data.Dates.dt.hour # Gets the hour portion form the \"Dates\" column\n",
    "    \n",
    "    def prep_day(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.DayOfWeek)\n",
    "    \n",
    "    def prep_years(self):\n",
    "        # beware: 2015 has significantly less incidents than the other years in this dataset.        \n",
    "        pass\n",
    "    \n",
    "    def concat_features(self):\n",
    "        hour = self.prep_hour()\n",
    "        day = self.prep_day()\n",
    "        district = self.prep_district()\n",
    "        return pd.concat([hour, day, district], axis=1)\n",
    "    \n",
    "    # Encode or decode classes\n",
    "    def encode_Y(self, Y):\n",
    "        return self.Y_encoder.fit_transform(Y)\n",
    "\n",
    "    def decode_Y(self, encoded_Y):\n",
    "        return self.Y_encoder.inverse_transform(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       0         0        0          1   \n",
       "1     23       0       0         0       0         0        0          1   \n",
       "2     23       0       0         0       0         0        0          1   \n",
       "3     23       0       0         0       0         0        0          1   \n",
       "4     23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sfp = SFP(train)\n",
    "X = sfp.concat_features()\n",
    "y = sfp.encode_Y(train.Category)\n",
    "\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note: this X and Y in particular are from the data in train.csv. See previous section for details.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data to the algorithm\n",
    "\n",
    "Decision trees are known to be good at handling categorical data. Let's try using some of the decision tree variations in scikit learn (decision tree, random forest, gradient boost, etc) and tweak some hyperparameters. We might even do some ensemble learning. Oooh shiny!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "_ = dtc.fit(X_train, y_train)\n",
    "y_predictions = dtc.predict_proba(X_test)\n",
    "dtc_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.007115993440352"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_log_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filippos says he thinks this log loss value of 3.007115993440352 is very deece. Confirmed by looking at the kaggle leaderboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Using some hyperparameter values from DataQuest mission 75\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=1) \n",
    "_ = rf.fit(X_train, y_train)\n",
    "y_predictions = rf.predict_proba(X_test)\n",
    "rf_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0111558103212417"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "[scikit learn cheatsheet advises us to look into SGD classifiers!](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "Understanding SGD:\n",
    "* yay Andrew Ng ML video\n",
    "* batch gradient descent (looks at all of the training examples in every iteration)\n",
    "* stochastic gradient descent (looks at only one training example in every iteration)\n",
    "    * how well is my hypothesis doing on a single example? for a given theta and x,y pair\n",
    "* different in the implementation details and making progress towards the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "_ = sgd.fit(X_train, y_train)\n",
    "y_predictions = sgd.predict_proba(X_test)\n",
    "sgd_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7003337842632695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\"  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb = BernoulliNB()\n",
    "_ = nb.fit(X_train, y_train)\n",
    "y_nb_predictions = nb.predict_proba(X_test)\n",
    "nb_log_loss = log_loss(y_test, y_nb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.609770164645326"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\" (again)  -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A baseline\n",
    "\n",
    "What's the log loss for a model that just predicts that all new data is the most common crime type ('LARCENCY/THEFT') ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = sfp.decode_Y(sgd.classes_)\n",
    "\n",
    "# replace with something more elegant later; had some weird things to debug \n",
    "for x in range(len(decoded)):\n",
    "    if decoded[x] == \"LARCENY/THEFT\":\n",
    "        popular_index = x\n",
    "\n",
    "# baseline_row = np.zeros(39)\n",
    "# baseline_row[popular_index] = 1\n",
    "\n",
    "baseline_y_pred = np.zeros((y_test.shape[0], 39))\n",
    "baseline_y_pred[:,16] = 1\n",
    "\n",
    "baseline_log_loss = log_loss(y_test, baseline_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.658162800015461"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this baseline model that assumes all unseen incidents are LARCENY/THEFT does terribly. This should be a helpful comparison for our classifiers above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "ASAP:\n",
    "* ~~prepare a submission to kaggle~~\n",
    "* Visualization of performance of the different models (include an ensemble learning result in here too)\n",
    "* ~~Comparison of log loss to the \"predict the most common thing\" baseline model (predict all unseen incidents to be LARCENY/THEFT)~~\n",
    "* *10 fold validation + mean and standard deviation for the metrics* - Emily\n",
    "* ~~log loss on each separate class (turn the multi class problem into 39 binary problems)~~\n",
    "    * --> discover more specifically which things the model predicts well and not so well \n",
    "    * --> more exploration and feature engineering in hopes of resolving the difference in performance between crime classes\n",
    "\n",
    "\n",
    "Backlog:\n",
    "* ~~explanation on SGD black box~~\n",
    "* ~~How to translate the 39-element outputs into more \"human readable\" outputs: TOP5~~\n",
    "* *Creative approaches to ensemble learning!* - Filippos \n",
    "* ~~Naive Bayes~~\n",
    "* Try playing with hyperparameters; see how changes in those values impact the logloss, and plot them (hyperparameter value vs. log loss)\n",
    "\n",
    "Process comments:\n",
    "* We're relatively happy with our current preprocessor to pause on the feature engineering and do experiments with the predictive models; we'll cycle back to the feature engineering if there's time and interest. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out on the test set\n",
    "\n",
    "Preparing our inputs (the X matrix): This should all look very familiar to the cells in the previous sections.\n",
    "\n",
    "Be careful of your variable names!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       0         0        0          1   \n",
       "1     23       0       0         0       0         0        0          1   \n",
       "2     23       0       0         0       0         0        0          1   \n",
       "3     23       0       0         0       0         0        0          1   \n",
       "4     23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       1         0        0          0   \n",
       "1     23       0       0         0       1         0        0          0   \n",
       "2     23       0       0         0       1         0        0          0   \n",
       "3     23       0       0         0       1         0        0          0   \n",
       "4     23       0       0         0       1         0        0          0   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        1        0          0        0         0     0         0         0   \n",
       "1        1        0          0        0         0     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          1        0         0     0         0         0   \n",
       "4        0        0          1        0         0     0         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sfp_submission will be used to preprocess the data from test.csv\n",
    "sfp_submission = SFP(test) \n",
    "X_submission = sfp_submission.concat_features() \n",
    "\n",
    "# Sanity check: These should not be the same, because X is from train.csv and X_submission is from test.csv\n",
    "display(X.head())\n",
    "display(X_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a first pass, usnig the sgd we trained earlier in the notebook\n",
    "y_predictions_submission = sgd.predict_proba(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.050622</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.071214</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.099993</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>4.122625e-07</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.011439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.050622</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.071214</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.099993</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>4.122625e-07</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.099175</td>\n",
       "      <td>0.018914</td>\n",
       "      <td>0.011439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.029403</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.070650</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.069939</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.008132</td>\n",
       "      <td>3.542536e-07</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.003955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.068561</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.064422</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>4.264281e-07</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.151420</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.008885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.068561</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.064422</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>4.264281e-07</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.012848</td>\n",
       "      <td>0.151420</td>\n",
       "      <td>0.012302</td>\n",
       "      <td>0.008885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.001809  0.050622  0.000521  0.000056  0.071214  0.001588  0.004362   \n",
       "1  0.001809  0.050622  0.000521  0.000056  0.071214  0.001588  0.004362   \n",
       "2  0.000600  0.029403  0.000476  0.000031  0.070650  0.001859  0.003960   \n",
       "3  0.001032  0.049713  0.000571  0.000051  0.068561  0.001479  0.004871   \n",
       "4  0.001032  0.049713  0.000571  0.000051  0.068561  0.001479  0.004871   \n",
       "\n",
       "         7         8         9     ...           29        30        31  \\\n",
       "0  0.099993  0.001679  0.000408    ...     0.000666  0.002166  0.000115   \n",
       "1  0.099993  0.001679  0.000408    ...     0.000666  0.002166  0.000115   \n",
       "2  0.069939  0.001730  0.000397    ...     0.000493  0.002711  0.000103   \n",
       "3  0.064422  0.001598  0.000447    ...     0.000717  0.002403  0.000130   \n",
       "4  0.064422  0.001598  0.000447    ...     0.000717  0.002403  0.000130   \n",
       "\n",
       "         32            33        34        35        36        37        38  \n",
       "0  0.014865  4.122625e-07  0.002346  0.010243  0.099175  0.018914  0.011439  \n",
       "1  0.014865  4.122625e-07  0.002346  0.010243  0.099175  0.018914  0.011439  \n",
       "2  0.008132  3.542536e-07  0.002078  0.007652  0.063125  0.015332  0.003955  \n",
       "3  0.014045  4.264281e-07  0.001857  0.012848  0.151420  0.012302  0.008885  \n",
       "4  0.014045  4.264281e-07  0.001857  0.012848  0.151420  0.012302  0.008885  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "df_submission = pd.DataFrame(y_predictions_submission)\n",
    "display(df_submission.head())\n",
    "filename = \"model5_sgd.csv\"\n",
    "# df_submission.to_csv(filename, index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(884262, 39)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape # should have 884262 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model received a log loss score of 2.70463 (rank 857 on the kaggle leaderboards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental notes on how we're preparing submission for kaggle\n",
    "\n",
    "* a header that describes the different categories... \n",
    "* id column (?)\n",
    "* each row is a 39-element vector (probablity for each of the 39 classes for each incident)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we make the header from our predict_proba output?\n",
    "\n",
    "According to the scikit learn documentation, the output is: \"The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_.\" [Also, thanks stackoverflow for an example.](http://stackoverflow.com/questions/16858652/how-to-find-the-corresponding-class-in-clf-predict-proba/16859091#16859091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.80885600e-03   5.06217664e-02   5.21420559e-04   5.55240732e-05\n",
      "   7.12139792e-02   1.58834381e-03   4.36221428e-03   9.99933111e-02\n",
      "   1.67876161e-03   4.07881317e-04   1.06878663e-04   1.71329406e-03\n",
      "   6.64602262e-04   7.70239747e-03   6.04011922e-04   1.10289901e-03\n",
      "   2.49413071e-01   1.09520936e-02   1.47554328e-03   1.47796173e-02\n",
      "   5.93034121e-02   1.98344334e-01   1.94216861e-05   1.17910028e-04\n",
      "   2.07942225e-03   9.16850137e-03   1.73778213e-03   4.65547214e-02\n",
      "   1.97833511e-03   6.65919728e-04   2.16619888e-03   1.14727301e-04\n",
      "   1.48648584e-02   4.12262470e-07   2.34649890e-03   1.02434863e-02\n",
      "   9.91745392e-02   1.89140145e-02   1.14390377e-02]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38]\n",
      "['ARSON' 'ASSAULT' 'BAD CHECKS' 'BRIBERY' 'BURGLARY' 'DISORDERLY CONDUCT'\n",
      " 'DRIVING UNDER THE INFLUENCE' 'DRUG/NARCOTIC' 'DRUNKENNESS' 'EMBEZZLEMENT'\n",
      " 'EXTORTION' 'FAMILY OFFENSES' 'FORGERY/COUNTERFEITING' 'FRAUD' 'GAMBLING'\n",
      " 'KIDNAPPING' 'LARCENY/THEFT' 'LIQUOR LAWS' 'LOITERING' 'MISSING PERSON'\n",
      " 'NON-CRIMINAL' 'OTHER OFFENSES' 'PORNOGRAPHY/OBSCENE MAT' 'PROSTITUTION'\n",
      " 'RECOVERED VEHICLE' 'ROBBERY' 'RUNAWAY' 'SECONDARY CODES'\n",
      " 'SEX OFFENSES FORCIBLE' 'SEX OFFENSES NON FORCIBLE' 'STOLEN PROPERTY'\n",
      " 'SUICIDE' 'SUSPICIOUS OCC' 'TREA' 'TRESPASS' 'VANDALISM' 'VEHICLE THEFT'\n",
      " 'WARRANTS' 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "print(y_predictions_submission[0])\n",
    "print(sgd.classes_) \n",
    "print(sfp.decode_Y(sgd.classes_)) # human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then prototyped some code to prepare the submission csv. Uncomment if you want to investigate further. (It's currently commented out to prevent the creation of a csv by accident.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making a test_submission \n",
    "# subset = y_predictions[0:10]\n",
    "# df = pd.DataFrame(subset)\n",
    "# submission_header = sfp.decode_Y(sgd.classes_).tolist()\n",
    "# csv = df.to_csv(\"test_submission.csv\", index=True, index_label=\"Id\", header=submission_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the outputs human readable\n",
    "\n",
    "Let's print out the top 5 probabilities for a single incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DRUG/NARCOTIC', 0.22952135351915204), ('OTHER OFFENSES', 0.15045548579077567), ('LARCENY/THEFT', 0.11885207094595898), ('ASSAULT', 0.10955692433372424), ('NON-CRIMINAL', 0.094745908698342901)]\n"
     ]
    }
   ],
   "source": [
    "a = y_predictions[0]\n",
    "\n",
    "# # strategy from http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "ind = np.argpartition(a, -5)[-5:]\n",
    "top5_words = sfp.decode_Y(ind)\n",
    "top5 = {}\n",
    "for i in range(len(ind)):\n",
    "    top5[top5_words[i]] = a[ind[i]] \n",
    "\n",
    "import operator\n",
    "sorted_top5 = sorted(top5.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print sorted_top5\n",
    "\n",
    "# Visualize for a given incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot category vs. probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling\n",
    "\n",
    "### Running several trials and summary statistics for those trials\n",
    "\n",
    "### Comparing different implementations (change in performance vs. frequency) \n",
    "\n",
    "### Examining model mistakes\n",
    "* Finding the differences between what it does well and what it does not as well\n",
    "\n",
    "### Making the outputs human readable\n",
    "* Top 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Class Log Loss\n",
    "\n",
    "We have a suspicion that our model is doing better on the classes which have more incidents. i.e. LARCENY/THEFT has many thousands more data points than ASSAULT. To investigate which classes our current multi-class model does poorly on, we are computing the per-class log loss for each class. \n",
    "\n",
    "This involves some processing of the y vectors into 1s and 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_nb_predictions\n",
    "\n",
    "# Computing the per-class log loss for class 16 (LARCENY/THEFT)\n",
    "y_test_per_class = pd.DataFrame({\n",
    "        \"class\" : y_test\n",
    "    })\n",
    "y_test_per_class.loc[y_test_per_class[\"class\"] == 16, \"class\"] = 1\n",
    "y_test_per_class.loc[y_test_per_class[\"class\"] != 16, \"class\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_class_log_loss = []\n",
    "for c in range(0, 39):\n",
    "    current_class = y_predictions[:, c]\n",
    "    not_current_class = 1 - y_predictions[:, c]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "            \"truth\" : y_test,\n",
    "            \"current_class\" : current_class,\n",
    "            \"not_current_class\" : not_current_class\n",
    "        })\n",
    "\n",
    "    df.loc[df[\"truth\"] == c, \"binary_truth\"] = 1\n",
    "    df.loc[df[\"truth\"] != c, \"binary_truth\"] = 0\n",
    "\n",
    "    y_binary_truth = np.array(df[\"binary_truth\"])\n",
    "    y_binary_predictions = np.array(pd.concat([df[\"not_current_class\"], df[\"current_class\"]], axis=1))\n",
    "\n",
    "    current_class_log_loss = log_loss(y_binary_truth, y_binary_predictions)\n",
    "    per_class_log_loss.append(current_class_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEDCAYAAADZUdTgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dffd8e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = sfp.decode_Y(nb.classes_).tolist()\n",
    "\n",
    "plt.scatter(range(0, 39), per_class_log_loss) # TODO: label axes, words on the x axis.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... this plot confuses us because the categories with the most data points (LARCENY/THEFT and ASSAULT) have the highest values??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "per_class_log_loss = []\n",
    "for c in range(0, 39):\n",
    "    current_class = y_predictions[:, c]\n",
    "    not_current_class = 1 - y_predictions[:, c]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "            \"truth\" : y_test,\n",
    "            \"current_class\" : current_class,\n",
    "            \"not_current_class\" : not_current_class\n",
    "        })\n",
    "\n",
    "    df.loc[df[\"truth\"] == c, \"binary_truth\"] = 1\n",
    "    df.loc[df[\"truth\"] != c, \"binary_truth\"] = 0\n",
    "\n",
    "    y_binary_truth = np.array(df[\"binary_truth\"])\n",
    "    y_binary_predictions = np.array(pd.concat([df[\"current_class\"], df[\"not_current_class\"]], axis=1))\n",
    "\n",
    "    current_class_log_loss = log_loss(y_binary_truth, y_binary_predictions)\n",
    "    per_class_log_loss.append(current_class_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEDCAYAAADKhpQUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dffda8810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(0, 39), per_class_log_loss) # TODO: label axes, words on the x axis.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's not get bogged down in the details... why did we have to flip the columns in binary predictions? Otherwise, the values we see on this plot make sense.\n",
    "\n",
    "It looks like our NB model is worst at predicting class 33 (TREA) and 22 (PORNOGRAHY/OBSCENE MAT) and is the best at predicting the most common classes 16 (LARCENY/THEFT) and 1 (ASSAULT). Classes 33 and 22 also happen to be the categories with the least amount of incidents in the training dataset. \n",
    "\n",
    "The next steps would be to explore how these classes are different from the other classes, and find different ways to represent these data points to make better predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also find summary statistics for the per-class log loss (what's the average log loss value? etc).\n",
    "\n",
    "log loss has been a tough error metric to grasp conceptually... I'm a bit shaky on what it means to have a high or low log loss score, comparing log loss scores to each other, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TREA', 'PORNOGRAPHY/OBSCENE MAT')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[33], labels[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the performance of our models\n",
    "\n",
    "We could...\n",
    "* Plot the log loss values of different classifiers\n",
    "* Compare training performance vs. kaggle performance\n",
    "* Compare classifiers with different hyperparameter values used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
