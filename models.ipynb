{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lines of code that apply the models to the test data and generate the submission file are commented out.  This is because the files created are >500MB each, and we didn't want somebody to accidentally create a bunch of large files if they ran all the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import crime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 28 columns):\n",
      "X                 878049 non-null float64\n",
      "Y                 878049 non-null float64\n",
      "Year              878049 non-null int64\n",
      "Month             878049 non-null int64\n",
      "Day               878049 non-null int64\n",
      "Hour              878049 non-null int64\n",
      "Minute            878049 non-null int64\n",
      "BogusReport       878049 non-null bool\n",
      "NBogusReport      878049 non-null bool\n",
      "DoW               878049 non-null int64\n",
      "Morning           878049 non-null int64\n",
      "Afternoon         878049 non-null int64\n",
      "Evening           878049 non-null int64\n",
      "Night             878049 non-null int64\n",
      "PdD               878049 non-null int64\n",
      "PdD_0             878049 non-null float64\n",
      "PdD_1             878049 non-null float64\n",
      "PdD_2             878049 non-null float64\n",
      "PdD_3             878049 non-null float64\n",
      "PdD_4             878049 non-null float64\n",
      "PdD_5             878049 non-null float64\n",
      "PdD_6             878049 non-null float64\n",
      "PdD_7             878049 non-null float64\n",
      "PdD_8             878049 non-null float64\n",
      "PdD_9             878049 non-null float64\n",
      "CategoryNumber    878049 non-null int64\n",
      "CornerCrime       878049 non-null int64\n",
      "ST_0              878049 non-null int64\n",
      "dtypes: bool(2), float64(12), int64(14)\n",
      "memory usage: 182.5 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 28 columns):\n",
      "Id              884262 non-null int64\n",
      "X               884262 non-null float64\n",
      "Y               884262 non-null float64\n",
      "Year            884262 non-null int64\n",
      "Month           884262 non-null int64\n",
      "Day             884262 non-null int64\n",
      "Hour            884262 non-null int64\n",
      "Minute          884262 non-null int64\n",
      "BogusReport     884262 non-null bool\n",
      "NBogusReport    884262 non-null bool\n",
      "DoW             884262 non-null int64\n",
      "Morning         884262 non-null int64\n",
      "Afternoon       884262 non-null int64\n",
      "Evening         884262 non-null int64\n",
      "Night           884262 non-null int64\n",
      "PdD             884262 non-null int64\n",
      "PdD_0           884262 non-null float64\n",
      "PdD_1           884262 non-null float64\n",
      "PdD_2           884262 non-null float64\n",
      "PdD_3           884262 non-null float64\n",
      "PdD_4           884262 non-null float64\n",
      "PdD_5           884262 non-null float64\n",
      "PdD_6           884262 non-null float64\n",
      "PdD_7           884262 non-null float64\n",
      "PdD_8           884262 non-null float64\n",
      "PdD_9           884262 non-null float64\n",
      "CornerCrime     884262 non-null int64\n",
      "ST_0            884262 non-null int64\n",
      "dtypes: bool(2), float64(12), int64(14)\n",
      "memory usage: 183.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train = crime.load_cleaned_train()\n",
    "test = crime.load_cleaned_test()\n",
    "\n",
    "print train.info()\n",
    "print test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is cleaned as described in `crime.py`.  In short, Year, Month, Day, Hour, and Minute columns are created, DayOfWeek, PdDistrict, and Category are encoded as integers, and invalid X and Y values are set to the median for that crime's PdDistrict.  In addition, PdDistrict information is one-hot encoded, a flag for crimes on a corner is added, and unnecessary columns are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split Train Data for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train\n",
    "y = X.pop('CategoryNumber')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `stratify` parameter of `train_test_split` requires scikit-learn-0.17, but ensures that the proportion of categories is maintained in the split.  The biggest thing that this does is make it so that we always get at least one crime from each category in the training set.  Our models can only predict based on what they have seen before, so it is crucial that we train them with all possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(alg, X_train, X_test, y_train, y_test):\n",
    "    predictor_sets = (\n",
    "        ['X', 'Y', 'CornerCrime', 'BogusReport', 'NBogusReport', 'ST_0'],\n",
    "        ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD', 'CornerCrime'],\n",
    "        ['Minute', 'Y', 'X', 'CornerCrime', 'Hour', 'PdD', 'Year', 'NBogusReport', 'Month'],\n",
    "        ['Minute', 'Y', 'X', 'CornerCrime', 'Hour', 'PdD', 'Year', 'NBogusReport', 'Month', 'BogusReport']\n",
    "    )\n",
    "\n",
    "    for predictors in predictor_sets:\n",
    "        alg.fit(X_train[predictors], y_train)\n",
    "        p = alg.predict_proba(X_test[predictors])\n",
    "        print crime.logloss(y_test, p), predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have something to compare to, we've created a baseline model that guesses based on the crime rates in each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class baseline(object):\n",
    "    def __init__(self):\n",
    "        self.has_fit = False\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = X_train.copy()\n",
    "        X_train['CategoryNumber'] = y_train\n",
    "        groups = X_train.groupby(['PdD', 'CategoryNumber'])\n",
    "\n",
    "        # Tally up the counts of each Category in each PdDistrict\n",
    "        num_districts = len(X_train.PdD.unique())\n",
    "        num_categories = len(y_train.unique())\n",
    "        self.district_rates = np.zeros((num_districts, num_categories))\n",
    "        for ind,data in groups:\n",
    "            self.district_rates[ind] = len(data)\n",
    "\n",
    "        # Normalize values\n",
    "        self.district_rates /= self.district_rates.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.has_fit = True\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        if self.has_fit:\n",
    "            predictions = X_test.PdD.apply(lambda x: self.district_rates[x,:])\n",
    "            return pd.DataFrame(predictions.tolist()).values  # to get a numpy array of the correct shape\n",
    "        return None\n",
    "\n",
    "alg = baseline()\n",
    "predictors = ['PdD']\n",
    "alg.fit(X_train[predictors], y_train)\n",
    "p = alg.predict_proba(X_test[predictors])\n",
    "print crime.logloss(y_test, p)\n",
    "\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'baseline_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.61645 on the test data, a very similar score to the cross validation.  This isn't too surprising since the way the train and test data are split up are by every other week, so our cross validation train-test split is pretty representative of the data as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we've chosen to try is the k-Nearest Neighbors model, partially for the fact that you can quite literally look at which crimes occurred near each other using the X and Y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.99210834292 ['X', 'Y']\n",
      "3.20051772488 ['DoW', 'Hour', 'Year']\n",
      "2.94271635483 ['X', 'Y', 'CornerCrime']\n",
      "3.00298633715 ['X', 'Y', 'Hour', 'PdD', 'CornerCrime']\n",
      "3.06264945443 ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD', 'CornerCrime']\n"
     ]
    }
   ],
   "source": [
    "alg = KNeighborsClassifier(n_neighbors=250)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model did the best when it only used spatial data, the X, Y, and CornerCrime columns.  It also gets better as `n_neighbors` is increased, but 250 is the maximum that I was able to do on my laptop.  I'll probably have to use less with the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'CornerCrime']\n",
    "alg = KNeighborsClassifier(n_neighbors=150)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'k-nn_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 3.40014 on the test data, but could probably have done better if run with a higher `n_neighbors` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to see how our old friend the Logistic Regression would do.  First, we should be able to replicate our baseline model by using a one-hot encoding of PdDistrict.  `C`, the inverse of regularization strength, is a factor that penalizes large coefficients.  Since our baseline model didn't do anything like this, we can set `C` to a very large number to make it have little effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [col for col in X_train.columns if 'PdD_' in col]\n",
    "alg = LogisticRegression(C=1e30)\n",
    "alg.fit(X_train[predictors], y_train)\n",
    "p = alg.predict_proba(X_test[predictors])\n",
    "print crime.logloss(y_test, p), predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty close to what our baseline model got, as we expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alg = LogisticRegression()\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model seems to do slightly better when it's given all of the predictors, but is still not as good as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = LogisticRegression()\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'lr_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.65839 on the test data, quite close to the cross validation score but still not as good as the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alg = tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try giving the model all the predictors since its scores are so close for just using two and using all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'dt_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.62696 on the test data.  Same story here:  very close to the cross validation score, but worse than our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'gb_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.69673 on the test data.  Again, no surprises here. Additionally, this model takes a very long time computationally to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46507713387 ['X', 'Y']\n",
      "2.65195846086 ['DoW', 'Hour', 'Year']\n",
      "2.43301324231 ['X', 'Y', 'CornerCrime']\n",
      "2.42163389311 ['X', 'Y', 'Hour', 'PdD', 'CornerCrime']\n",
      "2.44154444196 ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD', 'CornerCrime']\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more trees (`n_estimators`) seems to improve things, but makes a senior laptop with 8GB of RAM a bit sad.  If your computer can handle it, try increasing this parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Hour', 'PdD', 'CornerCrime']\n",
    "alg = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "crime.create_submission(alg, X, y, test, predictors, 'rf_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.41986 on the test data, better than the baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another notebook (`parameter_sweeps.ipynb`), we ran a variety of sweeps to try and tune our parameters.  The outcome of that was to use the features below as predictors, use as many estimators as our computers can handle (30 works for cross validation, but only 25 for the full dataset), and to set the max depth to 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41816255874 ['X', 'Y', 'CornerCrime', 'BogusReport', 'NBogusReport', 'ST_0']\n",
      "2.39390399362 ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD', 'CornerCrime']\n",
      "2.33125201384 ['Minute', 'Y', 'X', 'CornerCrime', 'Hour', 'PdD', 'Year', 'NBogusReport', 'Month']\n",
      "2.33390254332 ['Minute', 'Y', 'X', 'CornerCrime', 'Hour', 'PdD', 'Year', 'NBogusReport', 'Month', 'BogusReport']\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(n_estimators=30, max_depth=14, n_jobs=8)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['Minute', 'Y', 'X', 'CornerCrime', 'Hour', 'PdD', 'Year', 'NBogusReport', 'Month']\n",
    "alg = RandomForestClassifier(n_estimators=25, max_depth=14, n_jobs=4)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'rf_tuned_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.32886, our best yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telling a Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modelling exploration, we find that the methods which call out the categorical nature of the data - logistic regression, decision trees, and random forest classifiers - perform highly. Particularly relevant is that the predictors of location and time of day may be particularly telling, followed by year and day of week. In our exploration phase, these relationships were also evident. What we're seeing, at a high level, is that crime reporting or crime report filing over the past 10+ years in San Francisco has followed a similar pattern - the same types of crime are being committed with relatively similar frequency, in the same general locations, and are being reported by the expected reporting entity. \n",
    "\n",
    "What may be interesting to explore will be whether the type of crime that it is, different slices of time of year, or connecting reporting time and type of crime may yield more exacting results when predicting crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is such a simplistic, hard-coded model performing better than most of these scikit-learn models?  What is it about this dataset that makes these models perform as they do?  Would it be worth tweaking the parameters of one of these models to try and improve the score, or are they just the wrong models to be using for this problem?  How can we answer these questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
