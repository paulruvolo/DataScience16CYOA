{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Combine Models!\n",
    "\n",
    "### Joe and Keenan\n",
    "\n",
    "In this notebook, we are going to try combining our two most successful models in order to get a better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1500 Block of LOMBARD ST</td>\n",
       "      <td>-122.426995</td>\n",
       "      <td>37.800873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05-13 23:30:00</td>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>GRAND THEFT FROM LOCKED AUTO</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>PARK</td>\n",
       "      <td>NONE</td>\n",
       "      <td>100 Block of BRODERICK ST</td>\n",
       "      <td>-122.438738</td>\n",
       "      <td>37.771541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                      Descript  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
       "3 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "4 2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
       "\n",
       "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
       "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
       "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
       "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
       "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.425892  37.774599  \n",
       "1 -122.425892  37.774599  \n",
       "2 -122.424363  37.800414  \n",
       "3 -122.426995  37.800873  \n",
       "4 -122.438738  37.771541  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Load Data with pandas, and parse the first column into datetime\n",
    "train=pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test=pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "# Keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\n",
    "train = train[np.abs(train.X-train.X.mean())<=(3*train.X.std())] \n",
    "train = train[np.abs(train.Y-train.Y.mean())<=(3*train.Y.std())] \n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Process the Data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9  ...    MISSION  NORTHERN  PARK  RICHMOND  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...          0         1     0         0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...          0         1     0         0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...          0         1     0         0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...          0         1     0         0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...          0         0     1         0   \n",
      "\n",
      "   SOUTHERN  TARAVAL  TENDERLOIN           X          Y  crime  \n",
      "0         0        0           0 -122.425892  37.774599     37  \n",
      "1         0        0           0 -122.425892  37.774599     21  \n",
      "2         0        0           0 -122.424363  37.800414     21  \n",
      "3         0        0           0 -122.426995  37.800873     16  \n",
      "4         0        0           0 -122.438738  37.771541     16  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "#Convert crime labels to numbers\n",
    "le_crime = preprocessing.LabelEncoder()\n",
    "crime = le_crime.fit_transform(train.Category)\n",
    " \n",
    "#Get binarized weekdays, districts, and hours.\n",
    "days = pd.get_dummies(train.DayOfWeek)\n",
    "district = pd.get_dummies(train.PdDistrict)\n",
    "hour = train.Dates.dt.hour\n",
    "hour = pd.get_dummies(hour) \n",
    "x = train.X\n",
    "y = train.Y\n",
    " \n",
    "#Build new array\n",
    "train_data = pd.concat([hour, days, district, x, y], axis=1)\n",
    "train_data['crime']=crime\n",
    " \n",
    "#Repeat for test data\n",
    "days = pd.get_dummies(test.DayOfWeek)\n",
    "district = pd.get_dummies(test.PdDistrict)\n",
    " \n",
    "hour = test.Dates.dt.hour\n",
    "hour = pd.get_dummies(hour) \n",
    "x = test.X\n",
    "y = test.Y\n",
    " \n",
    "test_data = pd.concat([hour, days, district, x, y], axis=1)\n",
    "\n",
    "print train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN', 'X', 'Y', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday',\n",
    " 'Wednesday', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION',\n",
    " 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN', 'X', 'Y']\n",
    "\n",
    "# Add in hours of the day into the features\n",
    "features2 = [x for x in range(0,24)]\n",
    "features = features + features2\n",
    "\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <class 'pandas.core.frame.DataFrame'>\n",
      "(438991, 39)\n",
      "Naive Bayes:  2.58478544941\n"
     ]
    }
   ],
   "source": [
    "training, validation = train_test_split(train_data, train_size=.5)\n",
    "print type(training)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(training[features], training['crime'])\n",
    "predicted = np.array(model.predict_proba(validation[features]))\n",
    "print predicted.shape\n",
    "print \"Naive Bayes: \" , log_loss(validation['crime'], predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try combining features, the BernoulliNB and the Logistic Regression, two of the models that worked the best according to kaggle in the past. I'm using a similar method to the way we learned the ensemble from the Dataquest with the titanic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (438991, 39)\n",
      "Ensemble:  3.66356164613\n"
     ]
    }
   ],
   "source": [
    "training, validation = train_test_split(train_data, train_size=.5)\n",
    "\n",
    "algorithms = [[BernoulliNB(), features], [LogisticRegression(C=.01), features]]\n",
    "full_predictions = []\n",
    "\n",
    "for alg, features in algorithms:\n",
    "    filtered = training[features]\n",
    "    alg.fit(training[features], training['crime'])\n",
    "    predictions = np.array(alg.predict_proba(validation[features]))\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "predictions = (full_predictions[0] * 2 + full_predictions[1]) / 3\n",
    "predictions = predictions.astype(int)\n",
    "\n",
    "print predictions.shape\n",
    "print 'Ensemble: ', log_loss(validation['crime'], predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this resulting in a score of 3.663, which is pretty terrible. Somehow, combining the two features did not do particularily well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a Gradient Boosting Classifier!\n",
    "\n",
    "Because why not? Let's import from sklearn, choose some parameters that seem decent based on using this model from the titanic dataset, and running the model to see how the log loss compares to previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting!:  2.54999099019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "training, validation = train_test_split(train_data, train_size=.5)\n",
    "model=GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "model.fit(training[features], training['crime'])\n",
    "predicted = np.array(model.predict_proba(validation[features]))\n",
    "\n",
    "print \"Gradient Boosting!: \" , log_loss(validation['crime'], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Oh Yeah! That's a significant improvement over our last score. However, I remember last time during the titanic dataset, gradient boosting was very prone to overfitting, so I don't want to get too confident at this point. Anyways, lets make a kaggle submission and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "model.fit(training[features], training['crime'])\n",
    "predicted = model.predict_proba(test_data[features])\n",
    "result=pd.DataFrame(predicted, columns=le_crime.classes_)\n",
    "result.to_csv('testResultGradientBoosting.csv', index = True, index_label = 'Id' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yay! This model achieved a score of: 2.54949 on Kaggle, good enough for 425th place as of right now. It definitely took a lot longer to run, about 30-60 minutes. I would like to try a gridsearch with tuning parameters like random states, n_estimators (add more of them to make sure I'm not overfitting), or change the max depth. However, this is going to take way too long, unfortunetly. I am pleased with the improvement, however! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
