{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emily Wang and Filippos Lymperopoulos | Data Science 2016 | CYOA: sfcrime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 15, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process\n",
    "\n",
    "* Import libraries and training data\n",
    "* Feature engineering / preprocessing: \n",
    "    * Make \"useful\" combinations of features to give our model; \n",
    "    * Also encode categorical things in an intelligent way; \n",
    "    * Can choose to only use a subset of features if desired\n",
    "* Partition your data (cross-validation kfolds, etc)\n",
    "* Model fit\n",
    "* Make some predictions\n",
    "* Compute the logloss score\n",
    "* Reflect; iterate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's import some useful libraries and import the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Category</th>\n",
       "      <th>Descript</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Resolution</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>WARRANTS</td>\n",
       "      <td>WARRANT ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-13 23:53:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>OAK ST / LAGUNA ST</td>\n",
       "      <td>-122.425892</td>\n",
       "      <td>37.774599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-13 23:33:00</td>\n",
       "      <td>OTHER OFFENSES</td>\n",
       "      <td>TRAFFIC VIOLATION ARREST</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>ARREST, BOOKED</td>\n",
       "      <td>VANNESS AV / GREENWICH ST</td>\n",
       "      <td>-122.424363</td>\n",
       "      <td>37.800414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dates        Category                  Descript  DayOfWeek  \\\n",
       "0 2015-05-13 23:53:00        WARRANTS            WARRANT ARREST  Wednesday   \n",
       "1 2015-05-13 23:53:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "2 2015-05-13 23:33:00  OTHER OFFENSES  TRAFFIC VIOLATION ARREST  Wednesday   \n",
       "\n",
       "  PdDistrict      Resolution                    Address           X          Y  \n",
       "0   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "1   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST -122.425892  37.774599  \n",
       "2   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST -122.424363  37.800414  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "\n",
    "# Convert the Dates column of our provided data from string to datetime format.\n",
    "train = pd.read_csv('train.csv', parse_dates = ['Dates'])\n",
    "test = pd.read_csv('test.csv', parse_dates = ['Dates'])\n",
    "\n",
    "# Print the first 3 rows of the dataframe.\n",
    "display(train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Preprocessing\n",
    "\n",
    "Make a class that will:\n",
    "* Extract the time features we want to use for the model (e.g. year, season, month, day, etc.)\n",
    "* Encode categorical variables in a meaningful way: contains a preprocessor that can both transform and inverse_transform the categorical variables\n",
    "* Return a transformed dataframe to be given to the model\n",
    "* Maybe: allow for some flexibility with what is in the transformed dataframe (to iterate quickly) (e.g. choosing how many time features you want in this experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SFP = SFCrime Preprocessor\n",
    "class SFP():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.Y_encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # Prepare inputs\n",
    "    def prep_district(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.PdDistrict)\n",
    "    \n",
    "    def prep_hour(self):\n",
    "        # a continuous value from 0 to 23\n",
    "        return self.data.Dates.dt.hour # Gets the hour portion form the \"Dates\" column\n",
    "    \n",
    "    def prep_day(self):\n",
    "        # one hot encoding\n",
    "        return pd.get_dummies(self.data.DayOfWeek)\n",
    "    \n",
    "    def prep_years(self):\n",
    "        # beware: 2015 has significantly less incidents than the other years in this dataset.        \n",
    "        pass\n",
    "    \n",
    "    def concat_features(self):\n",
    "        hour = self.prep_hour()\n",
    "        day = self.prep_day()\n",
    "        district = self.prep_district()\n",
    "        return pd.concat([hour, day, district], axis=1)\n",
    "    \n",
    "    # Encode or decode classes\n",
    "    def encode_Y(self, Y):\n",
    "        return self.Y_encoder.fit_transform(Y)\n",
    "\n",
    "    def decode_Y(self, encoded_Y):\n",
    "        return self.Y_encoder.inverse_transform(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Friday</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>BAYVIEW</th>\n",
       "      <th>CENTRAL</th>\n",
       "      <th>INGLESIDE</th>\n",
       "      <th>MISSION</th>\n",
       "      <th>NORTHERN</th>\n",
       "      <th>PARK</th>\n",
       "      <th>RICHMOND</th>\n",
       "      <th>SOUTHERN</th>\n",
       "      <th>TARAVAL</th>\n",
       "      <th>TENDERLOIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dates  Friday  Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  \\\n",
       "0     23       0       0         0       0         0        0          1   \n",
       "1     23       0       0         0       0         0        0          1   \n",
       "2     23       0       0         0       0         0        0          1   \n",
       "3     23       0       0         0       0         0        0          1   \n",
       "4     23       0       0         0       0         0        0          1   \n",
       "\n",
       "   BAYVIEW  CENTRAL  INGLESIDE  MISSION  NORTHERN  PARK  RICHMOND  SOUTHERN  \\\n",
       "0        0        0          0        0         1     0         0         0   \n",
       "1        0        0          0        0         1     0         0         0   \n",
       "2        0        0          0        0         1     0         0         0   \n",
       "3        0        0          0        0         1     0         0         0   \n",
       "4        0        0          0        0         0     1         0         0   \n",
       "\n",
       "   TARAVAL  TENDERLOIN  \n",
       "0        0           0  \n",
       "1        0           0  \n",
       "2        0           0  \n",
       "3        0           0  \n",
       "4        0           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sfp = SFP(train)\n",
    "X = sfp.concat_features()\n",
    "y = sfp.encode_Y(train.Category)\n",
    "\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the training data to the algorithm\n",
    "\n",
    "Decision trees are known to be good at handling categorical data. Let's try using some of the decision tree variations in scikit learn (decision tree, random forest, gradient boost, etc) and tweak some hyperparameters. We might even do some ensemble learning. Oooh shiny!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dtc = tree.DecisionTreeClassifier()\n",
    "_ = dtc.fit(X_train, y_train)\n",
    "y_predictions = dtc.predict_proba(X_test)\n",
    "dtc_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.007115993440352"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_log_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filippos says he thinks this log loss value of 3.007115993440352 is very deece. Confirmed by looking at the kaggle leaderboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Using some hyperparameter values from DataQuest mission 75\n",
    "rf = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=4, min_samples_leaf=1) \n",
    "_ = rf.fit(X_train, y_train)\n",
    "y_predictions = rf.predict_proba(X_test)\n",
    "rf_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0111558103212417"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "[scikit learn cheatsheet advises us to look into SGD classifiers!](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "Understanding SGD:\n",
    "* yay Andrew Ng ML video\n",
    "* batch gradient descent (looks at all of the training examples in every iteration)\n",
    "* stochastic gradient descent (looks at only one training example in every iteration)\n",
    "    * how well is my hypothesis doing on a single example? for a given theta and x,y pair\n",
    "* different in the implementation details and making progress towards the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "_ = sgd.fit(X_train, y_train)\n",
    "y_predictions = sgd.predict_proba(X_test)\n",
    "sgd_log_loss = log_loss(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.785258088639869"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ooooooooooohhhh\" -- Emily and Filippos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "ASAP:\n",
    "* prepare a submission to kaggle\n",
    "* Try playing with hyperparameters; see how changes in those values impact the logloss, and plot them\n",
    "* Visualization of performance of the different models (include an ensemble learning result in here too)\n",
    "* 10 fold validation + mean and standard deviation for the metrics\n",
    "* log loss on each separate class (turn the multi class problem into 39 binary problems) \n",
    "    * --> discover more specifically which things the model predicts well and not so well \n",
    "    * --> more exploration and feature engineering in hopes of resolving the difference in performance between crime classes\n",
    "\n",
    "\n",
    "Backlog:\n",
    "* Ask Paul for more explanation on these ML black boxes\n",
    "* How to translate the 39-element outputs into more \"human readable\" outputs\n",
    "* Creative approaches to ensemble learning! \n",
    "* Additional helpful visualizations for finding suggestions on how to improve?\n",
    "* hyperparameter value vs. log loss \n",
    "* Bayes?\n",
    "* Comparison of log loss to the \"predict the most common thing\" baseline model\n",
    "\n",
    "Process comments:\n",
    "* We're relatively happy with our current preprocessor to pause on the feature engineering and do experiments with the predictive models; we'll cycle back to the feature engineering if there's time and interest. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feb 16, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it out on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing submission for kaggle\n",
    "\n",
    "* a header that describes the different categories... \n",
    "* id column (?)\n",
    "* each row is a 39-element vector (probablity for each of the 39 classes for each incident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.21343740e-04,   2.98906715e-02,   4.84143259e-05,\n",
       "         1.94533794e-04,   4.95664419e-02,   1.19028947e-03,\n",
       "         6.60663189e-04,   7.23089705e-02,   5.05239464e-02,\n",
       "         2.14417919e-04,   3.60190112e-05,   1.38082530e-04,\n",
       "         1.59421563e-03,   6.15019243e-03,   1.01847263e-04,\n",
       "         8.46573702e-04,   2.03196345e-01,   1.46608887e-04,\n",
       "         1.11703269e-03,   9.97055882e-03,   1.57795505e-02,\n",
       "         2.84973296e-01,   3.57805586e-06,   1.01998882e-03,\n",
       "         1.34313981e-04,   1.88087834e-02,   7.56271760e-05,\n",
       "         2.70269956e-03,   3.99986201e-04,   1.72779865e-05,\n",
       "         9.13980699e-04,   1.41591793e-04,   1.53593340e-02,\n",
       "         1.42200710e-07,   4.04599101e-03,   8.64644323e-03,\n",
       "         8.56649771e-03,   2.08950780e-01,   1.44296851e-03])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
       "       'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
       "       'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
       "       'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
       "       'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
       "       'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
       "       'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
       "       'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
       "       'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
       "       'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
       "       'WARRANTS', 'WEAPON LAWS'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How do we make the header from our predict_proba output?\n",
    "# According to the scikit learn documentation, the output is: \n",
    "# \"The class probabilities of the input samples. The order of the classes corresponds \n",
    "# to that in the attribute classes_.\"\n",
    "\n",
    "display(y_predictions[0])\n",
    "display(sgd.classes_) \n",
    "display(sfp.decode_Y(sgd.classes_)) # human readable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
