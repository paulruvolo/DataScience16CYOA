{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration - For Super Computer\n",
    "By Sophia and Mac I\n",
    "This notebook is meant to be run on deepthought. (As a result, we won't execute all the cells.) What we've done in this model is just increase the number/depth of trees that we are making with the random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib import cm\n",
    "from datetime import datetime\n",
    "from ipywidgets import widgets  \n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print \"imports done\"\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data\n",
    "Now, I need to load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readData = pd.read_csv('train.csv')\n",
    "\n",
    "print \"dataRead\"\n",
    "\n",
    "Categories = ['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY',\n",
    "              'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE',\n",
    "              'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION',\n",
    "              'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING',\n",
    "              'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING',\n",
    "              'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES',\n",
    "              'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE',\n",
    "              'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE',\n",
    "              'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE',\n",
    "              'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT',\n",
    "              'WARRANTS', 'WEAPON LAWS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for recoding data\n",
    "Here are the helper functions for recoding data. We'll add more as we create some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recodeData(df, isTrain = False):\n",
    "    '''This function takes in the dataframe that we get from loading in the \n",
    "    SF crime data and returns a re-coded dataframe that has all the \n",
    "    additional features we want to add and the categorical features recoded \n",
    "    and cleaned.\n",
    "    '''\n",
    "\n",
    "    #since the modifications are done in-place we don't return the dataframe. \n",
    "    #we do, however, return the list of all the columns we added.\n",
    "    df, newLatLon = removeOutlierLatLon(df)\n",
    "    df, newDate = recodeDates(df)\n",
    "    df, newDistrict = recodePoliceDistricts(df)\n",
    "    print \"recoding addresses\"\n",
    "    df, newAddress, streetColumns = recodeAddresses(df)\n",
    "\n",
    "    \n",
    "    addedColumns = [] \n",
    "    addedColumns += newDate\n",
    "    addedColumns += newDistrict \n",
    "    addedColumns += newLatLon\n",
    "    addedColumns += newAddress\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    if (isTrain):\n",
    "        newCategory = recodeCategories(df)\n",
    "        addedColumns += newCategory\n",
    "        try: #prevents error if the coumns have already been removed or we are processing test data\n",
    "            columnsToDrop = ['Descript', 'Resolution']\n",
    "            df.drop(columnsToDrop, axis=1, inplace=True)\n",
    "        except:\n",
    "            print \"already recoded or using test data\"\n",
    "         \n",
    "\n",
    "    return df, addedColumns, streetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeDates(df):\n",
    "    '''This function takes in a dataframe and recodes the date field into \n",
    "    useable values. Here, we also recode the day of week.'''\n",
    "    #Recode the dates column to year, month, day and hour columns\n",
    "    df['DateTime'] = df['Dates'].apply(\n",
    "        lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    df['Year'] = df['DateTime'].apply(lambda x: x.year)\n",
    "    df['Month'] = df['DateTime'].apply(lambda x: x.month)\n",
    "    df['Day'] = df['DateTime'].apply(lambda x: x.day)\n",
    "    df['Hour'] = df['DateTime'].apply(lambda x: x.hour)\n",
    "    df['Minute'] = df['DateTime'].apply(lambda x: x.minute)\n",
    "    df['DayOfWeekRecode'] = df['DateTime'].apply(lambda x: x.weekday())\n",
    "    #df['MinuteOfWeek'] = df['DateTime'].apply(lambda x: x.weekday()*24*60 + x.hour*60 + x.minute)\n",
    "\n",
    "    return df, ['Year', 'Month', 'Day', 'Hour', 'Minute', 'DayOfWeekRecode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodePoliceDistricts(df):\n",
    "    '''This function recodes the police district to a one-hot encoding \n",
    "    scheme.'''\n",
    "    districts = df['PdDistrict'].unique().tolist()\n",
    "    newColumns = []\n",
    "    for district in districts:\n",
    "        newColumns.append('District' + district)\n",
    "        df['District' + district] = df['PdDistrict'].apply(\n",
    "            lambda x: int(x == district))\n",
    "\n",
    "    return df, newColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recodeAddresses(df):\n",
    "    '''This function will attempt to create some features related to the address field in the database. To do this, \n",
    "    first, we need to split up the address field into two different address fields'''\n",
    "    \n",
    "    #If there are two addresss, split fields. Also extract the block number\n",
    "    df['Address1'] = df['Address'].apply(lambda x: re.sub(r'^\\d+ Block of ','',x.split(\" / \")[0]))\n",
    "    df['Address2'] = df['Address'].apply(lambda x: (x.split(\" / \")[1]) if (len(x.split(\" / \")) > 1) else '')\n",
    "    \n",
    "    streets = set(df['Address1'].unique().tolist() + df['Address2'].unique().tolist())\n",
    "    \n",
    "    streetColumns = []\n",
    "    i = 0\n",
    "    \n",
    "    print \"starting address dummy creation\"\n",
    "    \n",
    "    #address1Dummy = pd.get_dummies(df['Address1']).rename(columns=lambda x: str(x))\n",
    "    address1Dummy = pd.get_dummies(df['Address1'])\n",
    "    address1Dummy = address1Dummy.replace(0, np.nan)\n",
    "    \n",
    "    print \"completed address 1 dummy creation\"\n",
    "    address2Dummy = pd.get_dummies(df['Address2'])\n",
    "    address2Dummy = address2Dummy.replace(0, np.nan)\n",
    "    \n",
    "    print \"completed address 2 dummy creation\"\n",
    "    \n",
    "#     print address1Dummy\n",
    "#     print address2Dummy.info()\n",
    "    \n",
    "    mergedAddressDummy = address1Dummy.combine_first(address2Dummy)\n",
    "    \n",
    "    print \"completed address dummy DataFrames merge\"\n",
    "    mergedAddressDummy = mergedAddressDummy.fillna(0)\n",
    "    print \"completed fillna on mergedAddressDummy\"\n",
    "    \n",
    "    streetColumns = list(mergedAddressDummy.columns.values)\n",
    "    \n",
    "    df = pd.concat([df, mergedAddressDummy], axis=1)\n",
    "    print \"completed merge of original df and new dummy variable df\"\n",
    "    \n",
    "    print \"Dummy creation finished\"\n",
    "    \n",
    "#     for street in streets:\n",
    "#         df['OnStreet' + street] = df.apply(lambda x: (x['Address1'] == street or x['Address2'] == street), axis=1)\n",
    "#         streetColumns.append('OnStreet' + street)\n",
    "#         i += 1\n",
    "#         print i\n",
    "        \n",
    "\n",
    "    \n",
    "    df['BlockNumber'] = df['Address'].apply(lambda x: int(re.findall(r'^\\d+',x)[0]) if (len(re.findall(r'^\\d+',x)) > 0) else None )\n",
    "    df['BlockNumber'] = df['BlockNumber'].fillna(-1)\n",
    "    \n",
    "    #Also add the \"did the crime occur on a street corner field?\"\n",
    "    df['StreetCornerFlag'] = df['Address'].apply(lambda x: len(x.split(\" / \")) > 1)\n",
    "    \n",
    "    return df, ['StreetCornerFlag', 'BlockNumber'], streetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeOutlierLatLon(df):\n",
    "    '''This function will attempt remove outlier Latitudes and Longitudes'''\n",
    "    df.loc[df.X > -121, 'X'] = df.loc[(df.X > -121)].apply(lambda row: df.X[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "    df.loc[df.Y > 38, 'Y'] = df.loc[(df.Y > 38)].apply(lambda row: df.Y[df[\"PdDistrict\"] == row['PdDistrict']].median(), axis=1)\n",
    "\n",
    "    return df, ['X', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recodeCategories(df):\n",
    "    '''This function will attempt remove outlier Latitudes and Longitudes'''\n",
    "    #if 'Category' in df.columns:\n",
    "    df['CategoryRecode'] = df.Category.apply(lambda x: Categories.index(x))\n",
    "        \n",
    "    return df, ['CategoryRecode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recoding Columns\n",
    "Here, we want to do some recoding of the columns. To do this, we're going to use our handy-dandy helper functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crimeData, addedColumns, streetColumns = recodeData(\n",
    "    readData, isTrain = True)\n",
    "crimeData.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streetColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimeData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of the commone streets correstponding to each category of crime (in the same order as the list above) generated using the same method as in the exploring street data. We'll use those to learn just the one-hot encoding of those streets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commonStreets = ['FOLSOM ST',\n",
    " '16TH ST',\n",
    " 'JONES ST',\n",
    " 'TAYLOR ST',\n",
    " 'ARMSTRONG AV',\n",
    " 'EDDY ST',\n",
    " 'LARKIN ST',\n",
    " 'CASTRO ST',\n",
    " '10TH AV',\n",
    " '5TH ST',\n",
    " 'HAIGHT ST',\n",
    " 'OFARRELL ST',\n",
    " '11TH AV',\n",
    " 'PAGE ST',\n",
    " 'FITCH ST',\n",
    " 'CAPP ST',\n",
    " '13TH ST',\n",
    " '24TH AV',\n",
    " '17TH ST',\n",
    " '18TH ST',\n",
    " '19TH ST',\n",
    " 'GENEVA AV',\n",
    " 'GEARY BL',\n",
    " 'BRYANT ST',\n",
    " 'HYDE ST',\n",
    " '4TH ST',\n",
    " 'FULTON ST',\n",
    " 'LEAVENWORTH ST',\n",
    " 'COLE ST',\n",
    " 'ALEMANY BL',\n",
    " 'PHELPS ST',\n",
    " 'MISSION ST',\n",
    " '6TH ST',\n",
    " '12TH AV',\n",
    " 'SHOTWELL ST',\n",
    " 'TREAT AV',\n",
    " '7TH ST',\n",
    " 'JEFFERSON ST',\n",
    " 'QUESADA AV',\n",
    " 'TURK ST',\n",
    " '2ND ST',\n",
    " 'MARKET ST',\n",
    " 'GGBRIDGE HY',\n",
    " '24TH ST',\n",
    " 'CAPITOL AV',\n",
    " 'KEARNY ST',\n",
    " 'HARRISON ST',\n",
    " 'LYON ST',\n",
    " 'BUSH ST',\n",
    " 'POLK ST',\n",
    " '3RD ST',\n",
    " 'ELLIS ST',\n",
    " 'SOUTH VAN NESS AV',\n",
    " 'POTRERO AV',\n",
    " '20TH ST',\n",
    " 'POWELL ST']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration 1\n",
    "Now that I've done some recoding, I'm going to create my model. To do this, I'm going to do a random forest classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnsToUse = addedColumns\n",
    "\n",
    "columnsToUse = ['X','Y', 'Year', 'Month', 'Hour', 'Minute',\n",
    "       'DayOfWeekRecode', 'DistrictNORTHERN', 'DistrictPARK',\n",
    "       'DistrictINGLESIDE', 'DistrictBAYVIEW', 'DistrictRICHMOND',\n",
    "       'DistrictCENTRAL', 'DistrictTARAVAL', 'DistrictTENDERLOIN',\n",
    "       'DistrictMISSION', 'DistrictSOUTHERN', 'StreetCornerFlag', 'BlockNumber']\n",
    "\n",
    "\n",
    "\n",
    "X = crimeData[columnsToUse]\n",
    "y = crimeData['CategoryRecode']\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=400, max_depth = 20, random_state=1, n_jobs = 1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "k_folds = StratifiedShuffleSplit(y, 3, test_size=0.5, random_state=0)\n",
    "\n",
    "scores = []\n",
    "print \"starting kfold\"\n",
    "\n",
    "for k, (train, test) in enumerate(k_folds):\n",
    "    clf.fit(X.iloc[train], y.iloc[train])\n",
    "    probs = clf.predict_proba(X.iloc[test])\n",
    "    score = log_loss(y.iloc[test].values, probs)\n",
    "    print score\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores)\n",
    "print(\"Average: \" + str(np.average(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a submission file, we modified the submission function from [this script](https://www.kaggle.com/shifanmao/sf-crime/random-forest-2/code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(clf, Xtrain, ytrain, predictors, path='my_submission.csv'):\n",
    "    '''This function will take in a trained model, a list of predictors, and an optional \n",
    "    filepath and create a submissision file for us.'''\n",
    "   \n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    print \"test data loaded\"\n",
    "    \n",
    "    test_data, newColumns, streetColumns = recodeData(test_data)\n",
    "    print \"test data recoded\"\n",
    "    \n",
    "    testDataColumns = list(test_data.columns.values)\n",
    "    \n",
    "    existingPredictors = list(set(predictors) & set(testDataColumns))\n",
    "    \n",
    "    clf.fit(Xtrain[existingPredictors], ytrain)\n",
    "    print \"model fitted with all data\"\n",
    "    \n",
    "    #clf.fit(trainX[predictors], trainY)\n",
    "    predictions = clf.predict_proba(test_data[existingPredictors])\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'Id': test_data.Id\n",
    "    })\n",
    "    \n",
    "    for i in range(predictions.shape[1]):\n",
    "        submission[Categories[i]] = predictions[:,i]\n",
    "    submission.to_csv(path, index=False)\n",
    "\n",
    "    print(\" -- Wrote submission to file {}.\".format(path))\n",
    "    return existingPredictors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This model performed better than previous models, scoring 2.29871! Adding street information appears to have helped! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictorsUsed = make_submission(clf, X, y, columnsToUse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictorsUsedSaved = ['FOLSOM ST',\n",
    " '16TH ST',\n",
    " '13TH ST',\n",
    " 'TAYLOR ST',\n",
    " 'ARMSTRONG AV',\n",
    " 'EDDY ST',\n",
    " 'TREAT AV',\n",
    " 'LARKIN ST',\n",
    " 'CASTRO ST',\n",
    " 'StreetCornerFlag',\n",
    " '10TH AV',\n",
    " '5TH ST',\n",
    " 'HAIGHT ST',\n",
    " 'DistrictNORTHERN',\n",
    " 'OFARRELL ST',\n",
    " 'QUESADA AV',\n",
    " '11TH AV',\n",
    " 'DistrictPARK',\n",
    " 'PAGE ST',\n",
    " 'FITCH ST',\n",
    " 'DistrictTARAVAL',\n",
    " 'JONES ST',\n",
    " 'GEARY BL',\n",
    " '17TH ST',\n",
    " '18TH ST',\n",
    " '19TH ST',\n",
    " 'BRYANT ST',\n",
    " 'HYDE ST',\n",
    " '4TH ST',\n",
    " 'GENEVA AV',\n",
    " 'Minute',\n",
    " 'COLE ST',\n",
    " '7TH ST',\n",
    " 'DistrictBAYVIEW',\n",
    " 'ALEMANY BL',\n",
    " 'PHELPS ST',\n",
    " 'MISSION ST',\n",
    " 'Hour',\n",
    " '6TH ST',\n",
    " 'DistrictSOUTHERN',\n",
    " 'CAPP ST',\n",
    " 'SHOTWELL ST',\n",
    " '12TH AV',\n",
    " 'DistrictTENDERLOIN',\n",
    " 'DayOfWeekRecode',\n",
    " 'JEFFERSON ST',\n",
    " 'Month',\n",
    " 'Y',\n",
    " 'X',\n",
    " 'TURK ST',\n",
    " '2ND ST',\n",
    " 'MARKET ST',\n",
    " 'GGBRIDGE HY',\n",
    " 'DistrictCENTRAL',\n",
    " 'DistrictINGLESIDE',\n",
    " '24TH AV',\n",
    " '24TH ST',\n",
    " 'CAPITOL AV',\n",
    " 'KEARNY ST',\n",
    " 'HARRISON ST',\n",
    " 'FULTON ST',\n",
    " 'BUSH ST',\n",
    " 'POLK ST',\n",
    " 'DistrictRICHMOND',\n",
    " 'BlockNumber',\n",
    " '3RD ST',\n",
    " 'ELLIS ST',\n",
    " 'SOUTH VAN NESS AV',\n",
    " 'LEAVENWORTH ST',\n",
    " 'POTRERO AV',\n",
    " 'Year',\n",
    " '20TH ST',\n",
    " 'DistrictMISSION',\n",
    " 'POWELL ST',\n",
    " 'LYON ST']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Score: \t\n",
    "2.44156"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is us experimenting with grid search. We didn't find this particularly helpful, but it was fun to try out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "clfOpt = RandomForestClassifier(verbose=3)\n",
    "#(n_estimators=400, max_depth = 20, random_state=1, )\n",
    "\n",
    "param_dist = {\"max_depth\": [5, 10, 20 , None],\n",
    "              \"n_estimators\": [50, 200, 400, 800],\n",
    "              \"min_samples_split\": [2, 10, 100],\n",
    "              \"max_features\": ['auto', None]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 48\n",
    "random_search = RandomizedSearchCV(clfOpt, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, scoring=\"log_loss\", verbose=3, n_jobs = 12)\n",
    "\n",
    "finalX = X[predictorsUsedSaved].values.astype(np.float32)\n",
    "finaly = y.values.astype(int)\n",
    "print \"pre Random search finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"starting Randomized Search\"\n",
    "start = time()\n",
    "random_search.fit(finalX, finaly)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalXfloat = finalX.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalXfloat.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalXfloat.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We didn't really get much of an improvement with score using this, but maybe there are other sets of data that perform very well with a grid-search approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
