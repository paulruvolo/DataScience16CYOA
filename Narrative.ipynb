{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The San Francisco Crime Dataset and You\n",
    "## Staying safe in a city that's seen some stuff\n",
    "Whether it be the rising costs of rent, gentrification because of high tech companies, or any one of a number of different social issues in the city, San Francisco has been the center of many debates. Let's join the bandwagon and look into crime throughout the city!\n",
    "\n",
    "### MORE INTRO HERE\n",
    "\n",
    "To begin diving into this dataset, we started by importing some of the most used packages, such as Pandas to manipulate data, sklearn to build models, and numpy to do math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for building a model with BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "# for building a model with RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "# for validating the models\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "# used to create a submission file for Kaggle\n",
    "import gzip, csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, lots of imports. However, there are some noticeable additions there. The sklearn libraries are great for building and validating models, which is what we are going to dive into after some initial visualizations. More specifically, we are going to focus on building models with BernoulliNB and RandomForestClassifier. But first, let's import our data and have a peek at what may be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Dates        Category                      Descript  \\\n",
      "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
      "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "\n",
      "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
      "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
      "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
      "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.425892  37.774599  \n",
      "1 -122.425892  37.774599  \n",
      "2 -122.424363  37.800414  \n",
      "3 -122.426995  37.800873  \n",
      "4 -122.438738  37.771541  \n",
      "                   X              Y\n",
      "count  878049.000000  878049.000000\n",
      "mean     -122.422616      37.771020\n",
      "std         0.030354       0.456893\n",
      "min      -122.513642      37.707879\n",
      "25%      -122.432952      37.752427\n",
      "50%      -122.416420      37.775421\n",
      "75%      -122.406959      37.784369\n",
      "max      -120.500000      90.000000\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print train.head()\n",
    "print train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing that is important to notice is out `data.describe()` call only shows two columns, X and Y. This means that we are going to need to play a lot with our data if we want to make it understandable to a computer, and if we want to build a model from it. However, since the X and Y values are readily available to us, let us build a map to show where crimes happen in SF, which brings us to section 1:\n",
    "## Visualizations\n",
    "To start, let's build a map of crime in SF. (inspiration drawn from https://www.kaggle.com/victoregb/sf-crime/sf-pd-districts-and-top-10-crime-maps/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named basemap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c4a937ff20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmapdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sf_map_copyright_openstreetmap_contributors.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named basemap"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "mapdata = np.loadtxt(\"sf_map_copyright_openstreetmap_contributors.txt\")\n",
    "\n",
    "fig1 = plt.figure(figsize = (20,10))\n",
    "\n",
    "map = Basemap(projection='cyl',\n",
    "                  resolution = 'h',\n",
    "                  llcrnrlon = -122.52469,\n",
    "                  llcrnrlat = 37.69862,\n",
    "                  urcrnrlon = -122.33663,\n",
    "                  urcrnrlat = 37.82986,\n",
    "                  lat_0=37.752303,\n",
    "                  lon_0=-122.445576)\n",
    "\n",
    "plt.imshow(mapdata, cmap = plt.get_cmap('gray'),\n",
    "            extent=[-122.52469, -122.33663, 37.69862, 37.82986])\n",
    "\n",
    "x, y = train.X, train.Y\n",
    "map.scatter(x, y, 3, marker='.', color='b', alpha=0.25)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('map.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yikes. That's a lot of crime. Also, this shows us a bit about location and density, but refinement on our end could show us highest amounts of certain crime in specific areas, and other useful metrics. In this exploration however, we are more concerned with building a model, and while this map is pretty, it does not help much of our understanding.\n",
    "Looking at some other interesting metrics, it is worth noting the amount of crimes on each day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train.DayOfWeek.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friday may not be the best day to take the dog for a walk late at night. Looking at this more visually shows a similar idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "weekRects = ax.bar(np.arange(7), train.DayOfWeek.value_counts(), 0.35, color ='y')\n",
    "ax.set_ylabel('Number of Crimes')\n",
    "ax.set_xlabel('Days of Week')\n",
    "ax.set_xticks(np.arange(7), 0.55)\n",
    "ax.set_xticklabels(('Friday','Wednesday','Saturday','Thursday','Tuesday','Monday', 'Sunday'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, but not much difference between days of the week. Let's instead look at each hour and day of the week, with the hypothesis being that most crimes happen during the night. First we clean up the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "\n",
    "        >>> parse_date(\"2014-04-05 14:00:00\")\n",
    "        (2014, 4, 5, 14)\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "\n",
    "train['Hour'] = train.Dates.apply(lambda x: parse_date(x)[3])\n",
    "\n",
    "# map days of week to integers\n",
    "dow = {\n",
    "    'Monday':0,\n",
    "    'Tuesday':1,\n",
    "    'Wednesday':2,\n",
    "    'Thursday':3,\n",
    "    'Friday':4,\n",
    "    'Saturday':5,\n",
    "    'Sunday':6\n",
    "}\n",
    "train['DOW'] = train.DayOfWeek.map(dow)\n",
    "\n",
    "#take a count of Categories and put it in a numpy array so I can use it\n",
    "cats = pd.Series(data.Category.values.ravel()).unique()#make categories into a unique array\n",
    "cats.sort() #sort alphabetically so it can be used across days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with new columns of clean data, let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(6,4))\n",
    "plt.hist2d(\n",
    "    train.Hour.values,\n",
    "    train.DOW.values,\n",
    "    bins=[24,7],\n",
    "    range=[[-0.5,23.5],[-0.5,6.5]]\n",
    ")\n",
    "plt.xticks(np.arange(0,24,2))\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.yticks(np.arange(0,7),['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
    "plt.ylabel('Day of Week')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Crimes by Time and Day - All Categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! This gives us some ideas, mainly that it seems most crimes happen around 6pm on a Friday. One big thing that we are missing, however, is what crimes occur when. It's easy to say most crimes happen then, but what about white collar thefts, or carjacking? To look more into this, let's create subplots for each individual crime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2,figsize=(16,9))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "for i in np.arange(1,cats.size + 1):\n",
    "    ax = plt.subplot(5,8,i)\n",
    "    ax.set_title(cats[i - 1],fontsize=8)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    plt.hist2d(\n",
    "        train[train.Category==cats[i - 1]].Hour.values,\n",
    "        train[train.Category==cats[i - 1]].DOW.values, \n",
    "        bins=[24,7],\n",
    "        range=[[-0.5,23.5],[-0.5,6.5]]\n",
    "    )\n",
    "    plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice overview of all crimes. One interesting point is that most white collar crimes (embezzlement, forgery, fraud) happen around 12 noon. Weird, but that may just be that they were input into the system at noon because no specific time could be attributed to the crime.\n",
    "\n",
    "So at this point, we have an idea of when most crimes happen, a visualization on a map of this data, and we have started to clean the data. Let's quickly show visualizations for which district has the highest levels of crime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.PdDistrict.value_counts().plot(kind='bar', figsize=(8,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the number of cases per hour in each different district:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['event']=1\n",
    "hourly_district_events = train[['PdDistrict','Hour','event']].groupby(['PdDistrict','Hour']).count().reset_index()\n",
    "hourly_district_events_pivot = hourly_district_events.pivot(index='Hour', columns='PdDistrict', values='event').fillna(method='ffill')\n",
    "hourly_district_events_pivot.interpolate().plot(title='number of cases hourly by district', figsize=(10,6))\n",
    "plt.savefig('hourly_events_by_district.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the amount of crimes over hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly_events = train[['Hour','event']].groupby(['Hour']).count().reset_index()\n",
    "hourly_events.plot(kind='bar', figsize=(6, 6))\n",
    "plt.savefig('hourly_events.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The last graph adds little, but it is a great sanity check to show that most crimes happen more at night than the morning)\n",
    "With all these visualizations, we now have a bit of an idea of when crimes happen, where they happen, and possible hypotheses to explain why they happen (location location location). With this in mind, we now continue on to build a model, factoring in location and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Building\n",
    "As mentioned before, we are going to start building a model, with interest placed on location and day of week. Let's start by pruning our data (we are also going to load `train.csv` again, but clean it more thorougly. This is why we changed the variable name to `data`.\n",
    "\n",
    "With the method `cleanup()` we can make days of week useful for the model, along with police districts, dates, and X and Y locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "\n",
    "def cleanup(data):\n",
    "    dow = {\n",
    "        \"Monday\" : 0,\n",
    "        \"Tuesday\" : 1,\n",
    "        \"Wednesday\" : 2,\n",
    "        \"Thursday\" : 3,\n",
    "        \"Friday\" : 4,\n",
    "        \"Saturday\" : 5,\n",
    "        \"Sunday\" : 6\n",
    "    }\n",
    "    data[\"DOW\"] = data.DayOfWeek.map(dow)\n",
    "    pds = {\n",
    "        \"SOUTHERN\" : 0,\n",
    "        \"MISSION\" : 1,\n",
    "        \"NORTHERN\" : 2,\n",
    "        \"BAYVIEW\" : 3,\n",
    "        \"CENTRAL\" : 4,\n",
    "        \"TERNDERLOIN\" : 5,\n",
    "        \"INGLESIDE\" : 6,\n",
    "        \"TARAVAL\" : 7,\n",
    "        \"PARK\" : 8,\n",
    "        \"RICHMOND\" : 9\n",
    "    }\n",
    "    data[\"pds\"] = data.PdDistrict.map(pds)\n",
    "    # for crimes without PD, use \"Other\" : 10\n",
    "    data[\"pds\"] = data[\"pds\"].fillna(10)\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleaning train has no brakes! Let's apply this method to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data = cleanup(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so now we are going to do the predictive model. We drew inspiration from this script (https://www.kaggle.com/sonuk7/sf-crime/prediction-with-bernoulinb/code). Bernoulli distributions deal with discrete variables where we have a two-point distribution. The BernoulliNB method gives us the probability of an incident fitting a Category when the probability is trained with our input columns to be better than 50-50. We're using this because this data set deals a lot with categorical variables instead of a bunch of integers that could be continous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = data.Category.values\n",
    "cleanData = data.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"X\", \"Y\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(cleanData.dropna(), cats)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model, cleanData, data[\"Category\"], cv = 3)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then generate the test model and output that, but since we have 3 different iterations and the test model generator stays about the same, I can just tell you the score received with BernoulliNB was 2.66746. (More specifics can be found in the previous iteration notebooks)\n",
    "\n",
    "Concerning the score, 2.66746 is pretty good. It shows that BernoulliNB is a pretty good tool to use for fitting data, however we may want to overfit our data with more work, and see if we get a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Refinement: Dummy Variables\n",
    "\n",
    "Next, we are going to attempt to use dummy variables to define new columns for more of our data. Using dummies instead of reassignment could increase precision by separating the categorical variables. After all, days of the week aren't really on a continuous scale; it wasn't completely fair to map them as such. They're discrete instances where it either is or isn't a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanupDummies(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data = pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix = \"dow\")), axis = 1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix = \"pds\")), axis = 1)\n",
    "    return data\n",
    "\n",
    "data_dummies = pd.read_csv('train.csv')\n",
    "data_dummies = cleanupDummies(data_dummies)\n",
    "\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these dummy columns, let's create a new model with BernoulliNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BernoulliNB()\n",
    "cats = data.Category.values\n",
    "dataDrops = data.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "dummyDataDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model.fit(dataDrops.dropna(), cats)\n",
    "without_dummies_scores = cross_validation.cross_val_score(model, dataDrops, data[\"Category\"], cv = 3)\n",
    "\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "with_dummies_scores= cross_validation.cross_val_score(model, dummyDataDrops, data[\"Category\"], cv=3)\n",
    "\n",
    "with_dummies_scores.mean() - without_dummies_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like using dummies gives us a little improvement, which means this could be a good choice! We're going to go ahead and generate a test submission file for kaggle. This example is used throughout to generate all test files so for reference:\n",
    "\n",
    "## Generating Testing File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('bernoulinb.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gets us a score of 2.61102, which is a minor improvement. Let's continue refining with more cleaning, this time looking at the X and Y data itself. (some inspiration pulled from https://www.kaggle.com/c/sf-crime/forums/t/18853/feature-engineering-of-lat-long-x-y-helps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanupDummies(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    xy_scaler=preprocessing.StandardScaler()\n",
    "    xy_scaler.fit(data[[\"X\",\"Y\"]])\n",
    "    data[[\"X\",\"Y\"]]=xy_scaler.transform(data[[\"X\",\"Y\"]])\n",
    "    data[\"rot45_X\"]=0.707*data[\"Y\"]+0.707*data[\"X\"]\n",
    "    data[\"rot45_Y\"]=0.707*data[\"Y\"]-0.707*data[\"X\"]\n",
    "    data[\"rot30_X\"]=(1.732/2)*data[\"X\"]+(1./2)*data[\"Y\"]\n",
    "    data[\"rot30_Y\"]=(1.732/2)*data[\"Y\"]-(1./2)*data[\"X\"]\n",
    "    data[\"rot60_X\"]=(1./2)*data[\"X\"]+(1.732/2)*data[\"Y\"]\n",
    "    data[\"rot60_Y\"]=(1./2)*data[\"Y\"]-(1.732/2)*data[\"X\"]\n",
    "    data[\"radial_r\"]=np.sqrt(np.power(data[\"Y\"],2)+np.power(data[\"X\"],2))\n",
    "    \n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data =pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix=\"dow\")), axis=1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix=\"pds\")), axis=1)\n",
    "    return data\n",
    "\n",
    "data_dummies=pd.read_csv('train.csv')\n",
    "data_dummies= cleanupDummies(data_dummies)\n",
    "\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
