{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Models for Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a working notebook where Casey explores several models including Logistic regression, tfidf, and Stochatic Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas \n",
    "from pattern.en import *\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import pattern\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIV\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pandas.read_csv('../train.tsv', sep = '\\t') \n",
    "test = pandas.read_csv('../test.tsv', sep = '\\t')\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will now attempt to use Logistic Regression to train a model. \n",
    "Inspiration was pulled from this article: https://jessesw.com/NLP-Movie-Reviews/ \n",
    "Logistic Regression is a scikit learn model, documentation for scikit learn can be found here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, send ever word to lower and split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitPhrases(text):\n",
    "    punctuation = {\".\", \"/\", \"\\\\\", \",\"}\n",
    "    frequencyTracker = {}; \n",
    "    for mark in punctuation: \n",
    "        text = text.lower().replace(mark, \" \")\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##want the training sentiment values \n",
    "y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will go through and lower and split the data in order to make it ready for developing a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traindata = []\n",
    "for word in train['Phrase']: \n",
    "    traindata.append(\" \".join(splitPhrases(word)))\n",
    "    testdata = []\n",
    "    for otherword in test['Phrase']: \n",
    "        testdata.append(\" \".join(splitPhrases(otherword)))\n",
    "# for i in xrange(0,len(train['Phrase'])):\n",
    "#     print (\" \".join(splitPhrases(train['Phrase'][i])))\n",
    "#     traindata.append(\" \".join(splitPhrases(train['Phrase'][i])))\n",
    "#     testdata = []\n",
    "#     for i in xrange(0,len(test['Phrase'])):\n",
    "#         testdata.append(\" \".join(splitPhrases(test['Phrase'][i])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next we will use sklearn's TFIV library to vectorize the data. We made our own in class, so it would be interesting to compare the results if we use sklearn's TFIV compared to our own. Here is the documentation for the TFIV sklearn class: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "\n",
    "As you can see from the documentation, the parameters involved are: \n",
    "\"min-df\": this ignores the frequency of words less than a specific threshold value. \n",
    "\"max_features\": If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\"strip accents\": removes any ascii or unicode characters that are not words. \n",
    "\"analyzer\": could be either words or characters to analyze\n",
    "\"token pattern\": Regex denoting what constitutes a token or whateever as defined by analyzer\"\n",
    "\"ngram_range\": the lower and upper boundary for n gram parsing. An n gram is essentially spliting up the sentence into length n and making a model with these different data sets. iNstead of analyzing and training on each word or entire sentence it can be broken down into n grams. More info here: http://www.text-analytics101.com/2014/11/what-are-n-grams.html \n",
    "\"use_idf\": this allows the inverse document frequency to reweight itself. default is set to true. \n",
    "\"smooth_idf\": increases the document count for each word so that each word is at least in one document. This prevents dividing by 0. \n",
    "\"sublinear_tf\": sublinear scaling. Instead of log, use 1 + log(tf)\n",
    "\"stop_words\": words that were ignored because of their low frequency count, exceeded maximum counts, or occured in too few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TFIV(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will combine the train data and the test data in order to vectorize the data. \n",
    "After the tfv fits the data, then it transforms the documents into a document-terms matrix. \n",
    "Then seperates the data back into training set and testing set. \n",
    "\n",
    "Finally, then we are ready and able to use Logistic Regression on the vectorized document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = traindata + testdata # Combine both to fit the TFIDF vectorization.\n",
    "lentrain = len(traindata)\n",
    "\n",
    "tfv.fit(X_all) # This is the slow part!\n",
    "X_all = tfv.transform(X_all)\n",
    "\n",
    "X = X_all[:lentrain] # Separate back into training and test sets. \n",
    "X_test = X_all[lentrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parameters from scikit learn's Logistic Regression: \n",
    "\"C\": This is an optional float where the value is the inverse of the regularization strength. What is regularization strength you ask? Well here is what the internet says regularization strength is: \n",
    "    Regularization is applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting. When you train a model such as a logistic regression model, you are choosing parameters that give you the best fit to the data. This means minimizing the error between what the model predicts for your dependent variable given your data compared to what your dependent variable actually is.\n",
    "    The problem comes when you have a lot of parameters (a lot of independent variables) but not too much data. In this case, the model will often tailor the parameter values to idiosyncrasies in your data -- which means it fits your data almost perfectly. However because those idiosyncrasies don't appear in future data you see, your model predicts poorly.\n",
    "    \n",
    "\"dual\": This is a boolean where it's false if the number of samples > number of features and true for the opposite. \n",
    "In our case, this should be set to false because samples > features \n",
    "\n",
    "\"penalty\": Either choice of L1 or L2. Apparently the L1 norm is not differentiable, therefore the L2 norm will leave the data looking smoother. \n",
    "\n",
    "\"random-state\": when shuffling the data, this value will be the seed if trying to recreate it. \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_values = {'C':[30]} # Decide which settings you want for the grid search. \n",
    "\n",
    "model_LR = GridSearchCV(LR(penalty = 'L2', dual = True, random_state = 0), \n",
    "                        grid_values, scoring = 'roc_auc', cv = 20) \n",
    "# Try to set the scoring on what the contest is asking for. \n",
    "# The contest says scoring is for area under the ROC curve, so use this.\n",
    "# y_train = label_binarize(y_train, classes=[0, 1, 2, 3])\n",
    "\n",
    "model_LR.fit(X,y_train) # Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
