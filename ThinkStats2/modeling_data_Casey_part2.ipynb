{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#from textblob.classifiers import NaiveBayesClassifier\n",
    "#from nltk import NaiveBayesClassifier, classify\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading: ../train.tsv \n",
      "Writing: rotten.train.vw\n",
      "\n",
      "Reading: ../test.tsv \n",
      "Writing: rotten.test.vw\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "location_train = \"../train.tsv\"\n",
    "location_test = \"../test.tsv\"\n",
    "\n",
    "location_train_vw = \"rotten.train.vw\" #will be created\n",
    "location_test_vw = \"rotten.test.vw\" #will be created\n",
    "\n",
    "#cleans a string \"I'm a string!?\" returns as \"i m a string\"\n",
    "def clean(s):\n",
    "    return \" \".join(re.findall(r'\\w+', s,flags = re.UNICODE | re.LOCALE)).lower()\n",
    "\n",
    "#creates Vowpal Wabbit-formatted file from tsv file\n",
    "def to_vw(location_input_file, location_output_file, test = False):\n",
    "    print \"\\nReading:\",location_input_file,\"\\nWriting:\",location_output_file\n",
    "    with open(location_input_file) as infile, open(location_output_file, \"wb\") as outfile:\n",
    "        #create a reader to read train file\n",
    "        reader = csv.DictReader(infile, delimiter=\"\\t\")\n",
    "        #for every line\n",
    "        for row in reader:\n",
    "            #if test set label doesnt matter/or isnt available\n",
    "            if test:\n",
    "                label = \"1\"\n",
    "            else:\n",
    "                label = str(int(row['Sentiment'])+1)\n",
    "                phrase = clean(row['Phrase'])\n",
    "                outfile.write(   label +\n",
    "                              \" '\"+row['PhraseId'] + \n",
    "                              \" |f \" + phrase + \n",
    "                              \" |a \" + \n",
    "                              \"word_count:\"+str(phrase.count(\" \")+1)\n",
    "                              + \"\\n\" )\n",
    "                \n",
    "to_vw(location_train, location_train_vw)\n",
    "to_vw(location_test, location_test_vw, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-e5d4008a6200>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-e5d4008a6200>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    vw  rotten.train.vw -c -k --passes 300 --ngram 7 -b 24 --ect 5 -f rotten.model.vw\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "vw  rotten.train.vw -c -k --passes 300 --ngram 7 -b 24 --ect 5 -f rotten.model.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scikit_learn(train_set, train_labels):\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', OneVsOneClassifier(LinearSVC())),\n",
    "                     ])\n",
    "    X_train = np.asarray(train_set)\n",
    "    text_clf = text_clf.fit(X_train, np.asarray(train_labels))\n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pandas.read_csv('../train.tsv', sep='\\t')\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pandas.read_csv('../test.tsv', sep='\\t')\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Train the classifier\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "\n",
    "    ##############################TRAIN DATA#####################\n",
    "    with open('../train.tsv', 'r') as f:\n",
    "        sentimentreader = csv.reader(f, delimiter='\\t')\n",
    "        header = sentimentreader.next()\n",
    "        cnt = 0\n",
    "        for row in sentimentreader:\n",
    "            sentence = row[2]\n",
    "            sentiment = row[3]\n",
    "            if (cnt < 30):\n",
    "                test_set.append((sentence, sentiment))\n",
    "            elif (cnt > 30):\n",
    "                #cl.train(sentence, sentiment)\n",
    "                train_set.append((sentence, sentiment))\n",
    "            cnt += 1\n",
    "    \n",
    "    sentence_set = [sentence for sentence, label in train_set]\n",
    "    label_set = [int(label) for sentence, label in train_set]\n",
    "\n",
    "    test_sentence = [sentence for sentence, label in test_set]\n",
    "    label_test = [int(label) for sentence, label in test_set]\n",
    "\n",
    "    text_clf = scikit_learn(sentence_set, label_set)\n",
    "    \n",
    "    #Predict the test data\n",
    "    #predicted = text_clf.predict(test_sentence)\n",
    "    #print np.mean(predicted == np.asarray(label_test))\n",
    "    #for doc, category in zip(test_sentence, predicted):\n",
    "    #    print('%r => %s' % (doc, category))\n",
    "    #cl = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    #############################TEST DATA##################\n",
    "    \n",
    "    # Read test data and predict phrase based on train set\n",
    "    with open('../test.tsv', 'r') as f:\n",
    "        testreader = csv.reader(f, delimiter='\\t')\n",
    "        submission = open('scikit_submission.csv', 'w')\n",
    "        csvwriter = csv.writer(submission, delimiter=',')\n",
    "        csvwriter.writerow(['PhraseId', 'Sentiment'])\n",
    "        header = testreader.next()\n",
    "        phraseid_list = []\n",
    "        phrase_list = []\n",
    "        for row in testreader:\n",
    "            phraseid = row[0]\n",
    "            phrase = row[2]\n",
    "            phraseid_list.append(phraseid)\n",
    "            phrase_list.append(phrase)\n",
    "            #rating = cl.classify(phrase, default='0')\n",
    "            #write_row = [str(phraseid), str(rating)]\n",
    "            #csvwriter.writerow(write_row)\n",
    "        \n",
    "        predicted = text_clf.predict(np.asarray(phrase_list))\n",
    "        for i in range(len(predicted)):\n",
    "            sentiment_label = predicted[i]\n",
    "            phraseid = phraseid_list[i]\n",
    "            csvwriter.writerow([str(phraseid), str(sentiment_label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_train_test(data_train, data_test, train_ratio = 0.8, clean_string=True):\n",
    "    \"\"\"\n",
    "    Loads data and split into train and test sets.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    vocab = defaultdict(float)\n",
    "    # Pre-process train data set\n",
    "    for i in xrange(data_train.shape[0]):\n",
    "        line = data_train['Phrase'][i]\n",
    "        y = data_train['Sentiment'][i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': y, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': int(np.random.rand() < train_ratio)}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    # Pre-process test data set\n",
    "    for i in xrange(data_test.shape[0]):\n",
    "        line = data_test['Phrase'][i]\n",
    "        rev = []\n",
    "        rev.append(line.strip())\n",
    "        if clean_string:\n",
    "            orig_rev = clean_str(' '.join(rev))\n",
    "        else:\n",
    "            orig_rev = ' '.join(rev).lower()\n",
    "        words = set(orig_rev.split())\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum  = {'y': -1, \n",
    "                  'text': orig_rev,\n",
    "                  'num_words': len(orig_rev.split()),\n",
    "                  'split': -1}\n",
    "        revs.append(datum)\n",
    "        \n",
    "    return revs, vocab\n",
    "\n",
    "    \n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
    "    W[0] = np.zeros(k, dtype=np.float32)\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def load_bin_vec(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return word_vecs\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.    \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)  \n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n",
      "number of sentences: 222352\n",
      "vocab size: 17785\n",
      "max sentence length: 53\n",
      "loading word2vec vectors..."
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-e6a70afeeb76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'max sentence length: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'loading word2vec vectors...'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_bin_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'word2vec loaded!'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'num words already in word2vec: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-1ac328936473>\u001b[0m in \u001b[0;36mload_bin_vec\u001b[1;34m(fname, vocab)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \"\"\"\n\u001b[0;32m     66\u001b[0m     \u001b[0mword_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer1_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "w2v_file = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "revs, vocab = build_data_train_test(data_train, data_test, train_ratio=0.8, clean_string=True)\n",
    "max_l = np.max(pd.DataFrame(revs)['num_words'])\n",
    "print 'data loaded!'\n",
    "print 'number of sentences: ' + str(len(revs))\n",
    "print 'vocab size: ' + str(len(vocab))\n",
    "print 'max sentence length: ' + str(max_l)\n",
    "print 'loading word2vec vectors...',\n",
    "w2v = load_bin_vec(w2v_file, vocab)\n",
    "print 'word2vec loaded!'\n",
    "print 'num words already in word2vec: ' + str(len(w2v))\n",
    "add_unknown_words(w2v, vocab)\n",
    "W, word_idx_map = get_W(w2v)\n",
    "cPickle.dump([revs, W, word_idx_map, vocab], open('imdb-train-val-test.pickle', 'wb'))\n",
    "print 'dataset created!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
