{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas \n",
    "from pattern.en import *\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import pattern\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIV\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pandas.read_csv('../train.tsv', sep = '\\t') \n",
    "test = pandas.read_csv('../test.tsv', sep = '\\t')\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Logistic Regression to train a model.\n",
    "Inspiration was pulled from this article: https://jessesw.com/NLP-Movie-Reviews/ Logistic Regression is a scikit learn model, documentation for scikit learn can be found here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, clean the data by sending the word to lowercase, remoing all punctuation marks, and spliting the words at every space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanData(text):\n",
    "    punctuation = {\".\", \"/\", \"\\\\\", \",\"}\n",
    "    frequencyTracker = {}; \n",
    "    for mark in punctuation: \n",
    "        text = text.lower().replace(mark, \" \")\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#acqure all of the 'true' data, place all the data known to be true into array y_train\n",
    "y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calls the clean data function for both the training and test data. Then appends the cleaned data to array train and test data, respectively.\n",
    "traindata = []\n",
    "for word in train['Phrase']: \n",
    "    traindata.append(\" \".join(cleanData(word)))\n",
    "testdata = []\n",
    "for otherword in test['Phrase']: \n",
    "    testdata.append(\" \".join(cleanData(otherword)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use sklearn's TFIV library to vectorize the data and explore how often and how important a certain word occurs and is in the data set. This vectorization function that will later help determine which words are in positive reviews and which words are in negative reviews. Here is the documentation for the TFIV sklearn class: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "\n",
    "As you can see from the documentation, the parameters involved are: \n",
    "\"min-df\": this ignores the frequency of words less than a specific threshold value. \n",
    "\"max_features\": If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\"strip accents\": removes any ascii or unicode characters that are not words. \n",
    "\"analyzer\": could be either words or characters to analyze\n",
    "\"token pattern\": Regex denoting what constitutes a token or whateever as defined by analyzer\"\n",
    "\"ngram_range\": the lower and upper boundary for n gram parsing. An n gram is essentially spliting up the sentence into length n and making a model with these different data sets. iNstead of analyzing and training on each word or entire sentence it can be broken down into n grams. More info here: http://www.text-analytics101.com/2014/11/what-are-n-grams.html \n",
    "\"use_idf\": this allows the inverse document frequency to reweight itself. default is set to true. \n",
    "\"smooth_idf\": increases the document count for each word so that each word is at least in one document. This prevents dividing by 0. \n",
    "\"sublinear_tf\": sublinear scaling. Instead of log, use 1 + log(tf)\n",
    "\"stop_words\": words that were ignored because of their low frequency count, exceeded maximum counts, or occured in too few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfv = TFIV(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will combine the train data and the test data in order to vectorize the data. After the tfv fits the data, then it transforms the documents into a document-terms matrix. Then seperates the data back into training set and testing set in order to continue with the process. The tfv transform returns a sparse matrix.\n",
    "Finally, then we are ready and able to use Logistic Regression on the vectorized document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all = traindata + testdata # Combine both to fit the TFIDF vectorization.\n",
    "lentrain = len(traindata)\n",
    "\n",
    "tfv.fit(X_all) \n",
    "X_all = tfv.transform(X_all)\n",
    "\n",
    "X = X_all[:lentrain] # Separate back into training and test sets. \n",
    "X_test = X_all[lentrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 89472)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the shape of X, the sparse matrix, has more rows than columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that y_train is a 1D array of sentiment values."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parameters from scikit learn's Logistic Regression: \n",
    "\"C\": This is an optional float where the value is the inverse of the regularization strength. What is regularization strength you ask? Well here is what the internet says regularization strength is: \n",
    "    Regularization is applying a penalty to increasing the magnitude of parameter values in order to reduce overfitting. When you train a model such as a logistic regression model, you are choosing parameters that give you the best fit to the data. This means minimizing the error between what the model predicts for your dependent variable given your data compared to what your dependent variable actually is.\n",
    "    The problem comes when you have a lot of parameters (a lot of independent variables) but not too much data. In this case, the model will often tailor the parameter values to idiosyncrasies in your data -- which means it fits your data almost perfectly. However because those idiosyncrasies don't appear in future data you see, your model predicts poorly.\n",
    "    \n",
    "\"dual\": This is a boolean where it's false if the number of samples > number of features and true for the opposite. \n",
    "In our case, this should be set to false because samples > features \n",
    "\n",
    "\"penalty\": Either choice of L1 or L2. Apparently the L1 norm is not differentiable, therefore the L2 norm will leave the data looking smoother. \n",
    "\n",
    "\"random-state\": when shuffling the data, this value will be the seed if trying to recreate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casey/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/home/casey/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/home/casey/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n",
      "/home/casey/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:874: DeprecationWarning: penalty='L2' has been deprecated in favor of penalty='l2' as of 0.16. Backward compatibility for the uppercase notation will be removed in 0.18\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='L2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={'C': [30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_values = {'C':[30]} # this number is chosen for the reasoning above. More Cs!\n",
    "\n",
    "#Grid Search is an approximator for values that cannot be learned, but rather searches the space for the best fit. \n",
    "model_LR = GridSearchCV(LR(penalty = 'L2', dual = True, random_state = 0), \n",
    "                        grid_values) \n",
    "#fit the model \n",
    "model_LR.fit(X, y_train) # Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.53681, std: 0.00802, params: {'C': 30}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean scores, std, and params for all parameter combination of estimator functions\n",
    "model_LR.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=30, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='L2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is what the model predicts is the best estimator, this is like a line of best fit. \n",
    "#This is the estimator that gave the highest score in exhange for the smallest loss.\n",
    "model_LR.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, now we can predict the results of the test, put the results in a csv, and submit to kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the model to precist the scores of the test data\n",
    "#convert results to csv\n",
    "LR_result = model_LR.predict(X_test) # We only need the probabilities that the movie review was a 7 or greater. \n",
    "LR_output = pandas.DataFrame(data={\"PhraseId\":test[\"PhraseId\"], \"Sentiment\":LR_result}) # Create our dataframe that will be written.\n",
    "LR_output.to_csv('Logistic_Reg_Proj2.csv', index=False, quoting=3) # Get the .csv file we will submit to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading to kaggle, we discovered that the score was **0.58660**. This is interestingly low, but also close to our approximated, estimated mean. This was just an estimator model using linear regression on this estimation. We could have perhaps scored higher if we would have explored more with the scoring = 'roc_auc' attribute that belongs to GridSearchCV. This would just taken the area underneath the curve and perhaps provided a better approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
