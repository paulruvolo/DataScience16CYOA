{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "Date Created: 14 February 2016\n",
    "\n",
    "This model iteration is used to make crime category predictions for test data for San Francisco Crime Classification kaggle competition\n",
    "https://www.kaggle.com/c/sf-crime\n",
    "\n",
    "as of 14.02.16\n",
    "Rank: /\n",
    "Score: %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing train dataset\n",
    "z_train = zipfile.ZipFile('train.csv.zip')\n",
    "train = pd.read_csv(z_train.open('train.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing test dataset\n",
    "z_test = zipfile.ZipFile('test.csv.zip')\n",
    "test = pd.read_csv(z_test.open('test.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying and Trimming Data\n",
    "\n",
    "Here, we analyze data and modify it accordingly. As we see the data columns for the training and testing data, we see that the resolution column is not really needed. Moreover, some data types such as PdDistrict and Address seem to have some overlap, so we may pick to use one of them, or some altered version of each. Also, we dropped the Descript column from the train data will be dropped as there are great number of unique values, and are not present in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 9 columns):\n",
      "Dates         878049 non-null datetime64[ns]\n",
      "Category      878049 non-null object\n",
      "Descript      878049 non-null object\n",
      "DayOfWeek     878049 non-null object\n",
      "PdDistrict    878049 non-null object\n",
      "Resolution    878049 non-null object\n",
      "Address       878049 non-null object\n",
      "X             878049 non-null float64\n",
      "Y             878049 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), object(6)\n",
      "memory usage: 67.0+ MB\n",
      "None\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 7 columns):\n",
      "Id            884262 non-null int64\n",
      "Dates         884262 non-null datetime64[ns]\n",
      "DayOfWeek     884262 non-null object\n",
      "PdDistrict    884262 non-null object\n",
      "Address       884262 non-null object\n",
      "X             884262 non-null float64\n",
      "Y             884262 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(3)\n",
      "memory usage: 54.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print train.info()\n",
    "print \"----------------------------------\"\n",
    "print test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "The information in some of the columns in the data are extracted & seperated into different columns for better evaluation. \n",
    "\n",
    "The time_trim function converts total Date column into sub parts for convenience.\n",
    "\n",
    "The make_binary_fields function, allows us to create dummy variables with data from pre-existing columns. Dummy variables work extremely well with Random Forrest Regression, although the number of columns in the data set are increased. This will probably be used for randomforest or gradient boosting method.\n",
    "\n",
    "The make_seasons function converts month data into seasons which we can include in our approximation. However, seasons overlap with date greatly, so we may not fully utilize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modifying Functions\n",
    "\n",
    "From a source we used from the kaggle scripts, we create a streamlined function that adds new time categories to the data set based off of the 'Dates' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_trim(df):\n",
    "    df['Day'] = df['Dates'].dt.day\n",
    "    df['Month'] = df['Dates'].dt.month\n",
    "    df['Year'] = df['Dates'].dt.year\n",
    "    df['Hour'] = df['Dates'].dt.hour\n",
    "    df['Minute'] = df['Dates'].dt.minute\n",
    "    df['WeekOfYear'] = df['Dates'].dt.weekofyear\n",
    "    return\n",
    "\n",
    "def make_season(df):\n",
    "    \"\"\"\n",
    "    Make new field name Season\n",
    "    and binary fields for each season\n",
    "    Has to happen after making 'Month' field\n",
    "    spring: month 2, 3, 4\n",
    "    summer: month 5, 6, 7\n",
    "    autumn: month 8, 9, 10\n",
    "    winter: month 11, 12, 1\n",
    "    \"\"\"\n",
    "    df['Season'] = df['Month']\n",
    "    df.loc[(df['Season'] > 10) | (df['Season'] == 1), 'Season'] = 'Winter'\n",
    "    df.loc[(df['Season'] > 1) & (df['Season'] <= 4), 'Season'] = 'Spring'\n",
    "    df.loc[(df['Season'] > 4) & (df['Season'] <= 7), 'Season'] = 'Summer'\n",
    "    df.loc[(df['Season'] > 7) & (df['Season'] <= 10), 'Season'] = 'Autumn'\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data\n",
    "\n",
    "Here we alter the columns we have in our data set to suit our regression procedure. Because many of the sklearn regression functions cannot handle string data, we must convert them to either dummie variables or re-encode them with numbers. With the make_binary_fields and time_trim function, we do such for DayOfWeek, PdDistrict, and with LabelEncoder() we do such for the Categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_trim(train)\n",
    "seasons = make_season(train)\n",
    "\n",
    "time_trim(test)\n",
    "seasons = make_season(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Making into binary\n",
    "train = pd.concat((train, pd.get_dummies(train['DayOfWeek'], prefix = 'dow')), axis=1)\n",
    "train = pd.concat((train, pd.get_dummies(train['Season'], prefix = 'season')), axis=1)\n",
    "train = pd.concat((train, pd.get_dummies(train['PdDistrict'], prefix = 'pdd')), axis=1)\n",
    "\n",
    "test = pd.concat((test, pd.get_dummies(test['DayOfWeek'], prefix = 'dow')), axis=1)\n",
    "test = pd.concat((test, pd.get_dummies(test['Season'], prefix = 'season')), axis=1)\n",
    "test = pd.concat((test, pd.get_dummies(test['PdDistrict'], prefix = 'pdd')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Encoding into numbers\n",
    "def enc(df):\n",
    "    enc_pdd = LabelEncoder()\n",
    "    df['PdDistrict_enc'] = enc_pdd.fit_transform(df['PdDistrict'])\n",
    "\n",
    "    enc_seas = LabelEncoder()\n",
    "    df['Season_enc'] = enc_seas.fit_transform(df['Season'])\n",
    "\n",
    "    enc_dow = LabelEncoder()\n",
    "    df['DayOfWeek_enc'] = enc_dow.fit_transform(df['DayOfWeek'])\n",
    "    return\n",
    "\n",
    "enc(train)\n",
    "enc(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(train['Category'])\n",
    "train['CategoryEncoded'] = enc.transform(train['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 41 columns):\n",
      "Dates              878049 non-null datetime64[ns]\n",
      "Category           878049 non-null object\n",
      "Descript           878049 non-null object\n",
      "DayOfWeek          878049 non-null object\n",
      "PdDistrict         878049 non-null object\n",
      "Resolution         878049 non-null object\n",
      "Address            878049 non-null object\n",
      "X                  878049 non-null float64\n",
      "Y                  878049 non-null float64\n",
      "Day                878049 non-null int64\n",
      "Month              878049 non-null int64\n",
      "Year               878049 non-null int64\n",
      "Hour               878049 non-null int64\n",
      "Minute             878049 non-null int64\n",
      "WeekOfYear         878049 non-null int64\n",
      "Season             878049 non-null object\n",
      "dow_Friday         878049 non-null float64\n",
      "dow_Monday         878049 non-null float64\n",
      "dow_Saturday       878049 non-null float64\n",
      "dow_Sunday         878049 non-null float64\n",
      "dow_Thursday       878049 non-null float64\n",
      "dow_Tuesday        878049 non-null float64\n",
      "dow_Wednesday      878049 non-null float64\n",
      "season_Autumn      878049 non-null float64\n",
      "season_Spring      878049 non-null float64\n",
      "season_Summer      878049 non-null float64\n",
      "season_Winter      878049 non-null float64\n",
      "pdd_BAYVIEW        878049 non-null float64\n",
      "pdd_CENTRAL        878049 non-null float64\n",
      "pdd_INGLESIDE      878049 non-null float64\n",
      "pdd_MISSION        878049 non-null float64\n",
      "pdd_NORTHERN       878049 non-null float64\n",
      "pdd_PARK           878049 non-null float64\n",
      "pdd_RICHMOND       878049 non-null float64\n",
      "pdd_SOUTHERN       878049 non-null float64\n",
      "pdd_TARAVAL        878049 non-null float64\n",
      "pdd_TENDERLOIN     878049 non-null float64\n",
      "PdDistrict_enc     878049 non-null int64\n",
      "Season_enc         878049 non-null int64\n",
      "DayOfWeek_enc      878049 non-null int64\n",
      "CategoryEncoded    878049 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(23), int64(10), object(7)\n",
      "memory usage: 281.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting predictors\n",
    "\n",
    "The predictors are the data columns we will want to use for our regression process. We added the time components, and appended the PdDistric, Seasons, and day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['Day','Month','Year','Hour','Minute','WeekOfYear']\n",
    "predictors_b = ['pdd_TENDERLOIN', 'pdd_TARAVAL', 'pdd_SOUTHERN', 'pdd_RICHMOND', 'pdd_PARK', \n",
    "                'pdd_NORTHERN', 'pdd_MISSION', 'pdd_INGLESIDE', 'pdd_CENTRAL', 'pdd_BAYVIEW',\n",
    "                'season_Winter', 'season_Summer', 'season_Spring', 'season_Autumn', \n",
    "                'dow_Friday', 'dow_Monday', 'dow_Tuesday', 'dow_Wednesday', 'dow_Thursday', \n",
    "                'dow_Saturday', 'dow_Sunday']\n",
    "predictors_enc = ['PdDistrict_enc', 'Season_enc', 'DayOfWeek_enc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle SF Crime Classification Scoring System\n",
    "\n",
    "We noticed on the leader board of the SF Crime Kaggle the scoring system was logloss. Therefore, based upon someone else's algorithm on calculating log loss we developed our own. Here we initially get rid of extreme values (by designating an epsilon) and then create a log loss return that is our score.\n",
    "\n",
    "The log loss sytem works differently from the regular percentage scoring system. When computing the output for the test regression, each category (the point of interest) will be assigned a probability based upon the regression. With that, the solution is compared to the output of the test regression, and the total loss (or inaccuracy of each probability) is computed and summed. Overall, we want a lower log loss value for a more successful regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(y,p):\n",
    "    \"\"\"\n",
    "    information derived from following sources\n",
    "    https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "    https://www.kaggle.com/c/sf-crime/details/evaluation\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    p = p/p.sum(axis=1)[:,np.newaxis]\n",
    "    p = np.maximum(eps,p)\n",
    "    p = np.minimum(1-eps,p)\n",
    "    \n",
    "\n",
    "    # Calculate logloss\n",
    "    ll = 0\n",
    "    for i in range(len(p)):\n",
    "        ll += np.log(p[i, y.iloc[i]])\n",
    "    ll /= float(-len(p))\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating train data\n",
    "\n",
    "Here, we create folds in our data to split our training data into a train set and a test set. With this, we can test within our train data to see how accurate our model is before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create x and y from train data, x will be train, y will be target\n",
    "x_b = train[predictors + predictors_b]\n",
    "x_e = train[predictors + predictors_enc]\n",
    "y = train['CategoryEncoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtr_b, xtest_b, ytr_b, ytest_b = cross_validation.train_test_split(x_b, y, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtr_e, xtest_e, ytr_e, ytest_e = cross_validation.train_test_split(x_e, y, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Our first regression attempt is the logistic regression. We are familiar with this process from our warm up project and it is a good starting point. However, I believe that since the point of interest (category) has 38 parts, the logistic regression may not be apt, as during the Warmup Project the output was either alive or dead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.579205451592236"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = LogisticRegression()\n",
    "alg.fit(xtr_b, ytr_b)\n",
    "prediction = alg.predict_proba(xtest_b)\n",
    "logloss(ytest_b,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6269253551422671"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = LogisticRegression()\n",
    "alg.fit(xtr_e, ytr_e)\n",
    "prediction = alg.predict_proba(xtest_e)\n",
    "logloss(ytest_e,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = GradientBoostingClassifier()\n",
    "alg.fit(xtr_b, ytr_b)\n",
    "prediction = alg.predict_proba(xtest_b)\n",
    "logloss(ytest_b,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = GradientBoostingClassifier()\n",
    "alg.fit(xtr_e, ytr_e)\n",
    "prediction = alg.predict_proba(xtest_e)\n",
    "logloss(ytest_e,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5062227345094228"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = RandomForestClassifier(max_depth=13)\n",
    "alg.fit(xtr_b, ytr_b)\n",
    "prediction = alg.predict_proba(xtest_b)\n",
    "logloss(ytest_b,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4998721955608434"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = RandomForestClassifier(max_depth=12, min_samples_split=4, min_samples_leaf=5)\n",
    "alg.fit(xtr_b, ytr_b)\n",
    "prediction = alg.predict_proba(xtest_b)\n",
    "logloss(ytest_b,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5603905655630932"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = RandomForestClassifier(max_depth=4)\n",
    "alg.fit(xtr_e, ytr_e)\n",
    "prediction = alg.predict_proba(xtest_e)\n",
    "logloss(ytest_e,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5603264644083668"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = RandomForestClassifier(max_depth=4, min_samples_split=2, min_samples_leaf=2)\n",
    "alg.fit(xtr_e, ytr_e)\n",
    "prediction = alg.predict_proba(xtest_e)\n",
    "logloss(ytest_e,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "The decision tree uses a lot of conditional statements. Because we moved a lot of our categories to dummie variables, (with either true or false conditions), we believe that the decision tree model will be very suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5430288925085018"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = sk.tree.DecisionTreeClassifier(max_depth = 6)\n",
    "alg.fit(xtr_b, ytr_b)\n",
    "prediction = alg.predict_proba(xtest_b)\n",
    "logloss(ytest_b,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Playing around with min_samples_split and min_samples_leaf did not change the result much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(alg, feature_names=xtr_b.columns, out_file='tree_b.dot')\n",
    "\n",
    "%load_ext gvmagic\n",
    "f = open('tree_b.dot')\n",
    "tree_model_visualization = f.read()\n",
    "f.close()\n",
    "%dotstr tree_model_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5567671980169173"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = sk.tree.DecisionTreeClassifier(max_depth=4)\n",
    "alg.fit(xtr_e, ytr_e)\n",
    "prediction = alg.predict_proba(xtest_e)\n",
    "logloss(ytest_e,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_graphviz(alg, feature_names=xtr_e.columns, out_file='tree_e.dot')\n",
    "\n",
    "%reload_ext gvmagic\n",
    "f = open('tree_e.dot')\n",
    "tree_model_visualization = f.read()\n",
    "f.close()\n",
    "%dotstr tree_model_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_crimes = set(train.Category)\n",
    "crime_probabilities = {}\n",
    "for c in all_crimes:\n",
    "    crime_probabilities[c] = (train.Category == c).sum() / float(len(train.Category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(crime_probabilities, index = range(len(test)))\n",
    "submission['Id'] = test['Id']\n",
    "submission.to_csv('sfcc_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Kaggle Submission Result: 2.68016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting for test data using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = RandomForestClassifier(max_depth=12, min_samples_split=4, min_samples_leaf=5)\n",
    "alg.fit(x_b, y)\n",
    "prediction = alg.predict_proba(test[predictors + predictors_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'Id': test.Id})\n",
    "for i in range(prediction.shape[1]):\n",
    "    category = enc.inverse_transform([i])\n",
    "    submission[category[0]] = prediction[:,i]\n",
    "\n",
    "submission.to_csv('sfcc_rfc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Submission Result: 2.49867 (improvement of 0.18149 from baseline model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
