{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 3\n",
    "I want to implement a new method of XY coordinate cleaning here. Let's see if this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to keep using Dummies from iteration 2. Most of the changes to XY coords will be done in the cleanup function. I'm basing my changes off this script: https://www.kaggle.com/c/sf-crime/forums/t/18853/feature-engineering-of-lat-long-x-y-helps\n",
    "Basically, it uses feature engineering to make the coordinate use more intuitive. sklearn.preprocessing.StandardScaler scales the XY coords so that its variance and magnitude can't dwarf the other variables' features. With scaling, the XY coords are less likely to dominate the objective function. \n",
    "\n",
    "I also created new cartesian coordinate systems that are rotated around SF and a polar coordinate system. All of these center on the SF area and convey more relevant spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Dates', u'Category', u'Descript', u'DayOfWeek', u'PdDistrict',\n",
       "       u'Resolution', u'Address', u'X', u'Y', u'rot45_X', u'rot45_Y',\n",
       "       u'rot30_X', u'rot30_Y', u'rot60_X', u'rot60_Y', u'radial_r', u'Year',\n",
       "       u'Month', u'Hour', u'dow_Friday', u'dow_Monday', u'dow_Saturday',\n",
       "       u'dow_Sunday', u'dow_Thursday', u'dow_Tuesday', u'dow_Wednesday',\n",
       "       u'pds_BAYVIEW', u'pds_CENTRAL', u'pds_INGLESIDE', u'pds_MISSION',\n",
       "       u'pds_NORTHERN', u'pds_PARK', u'pds_RICHMOND', u'pds_SOUTHERN',\n",
       "       u'pds_TARAVAL', u'pds_TENDERLOIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "\n",
    "def cleanupDummies(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    xy_scaler=preprocessing.StandardScaler()\n",
    "    xy_scaler.fit(data[[\"X\",\"Y\"]])\n",
    "    data[[\"X\",\"Y\"]]=xy_scaler.transform(data[[\"X\",\"Y\"]])\n",
    "    data[\"rot45_X\"]=0.707*data[\"Y\"]+0.707*data[\"X\"]\n",
    "    data[\"rot45_Y\"]=0.707*data[\"Y\"]-0.707*data[\"X\"]\n",
    "    data[\"rot30_X\"]=(1.732/2)*data[\"X\"]+(1./2)*data[\"Y\"]\n",
    "    data[\"rot30_Y\"]=(1.732/2)*data[\"Y\"]-(1./2)*data[\"X\"]\n",
    "    data[\"rot60_X\"]=(1./2)*data[\"X\"]+(1.732/2)*data[\"Y\"]\n",
    "    data[\"rot60_Y\"]=(1./2)*data[\"Y\"]-(1.732/2)*data[\"X\"]\n",
    "    data[\"radial_r\"]=np.sqrt(np.power(data[\"Y\"],2)+np.power(data[\"X\"],2))\n",
    "    \n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data =pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix=\"dow\")), axis=1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix=\"pds\")), axis=1)\n",
    "    return data\n",
    "\n",
    "data_dummies=pd.read_csv('train.csv')\n",
    "data_dummies= cleanupDummies(data_dummies)\n",
    "\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these cross validation trials with dummies and without dummies to look at how much it may have helped our score. We'll also be uploading a test csv to kaggle. We're keeping with BernoulliNB for now because it predicts more accurately than the sample submission. Our next iteration will focus on algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score with all additions: 0.195751107956\n",
      "with only new coordinate systems: 0.197247620754\n",
      "scaled xy only score: 0.210547522574\n",
      "with only the xy coordinates: 0.220478593273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "model = BernoulliNB()\n",
    "cats = data_dummies.Category.values\n",
    "dummyDataDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "xyCordsDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"X\",\"Y\"], axis=1)\n",
    "\n",
    "newCordsDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"radial_r\"], axis=1)\n",
    "\n",
    "polarCordsOnly = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"X\", \"Y\"], axis=1)\n",
    "\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "with_dummies_scores= cross_validation.cross_val_score(model, dummyDataDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(xyCordsDrops.dropna(), cats)\n",
    "without_xy_scores= cross_validation.cross_val_score(model, xyCordsDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(newCordsDrops.dropna(), cats)\n",
    "without_newcords_scores= cross_validation.cross_val_score(model, newCordsDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(polarCordsOnly.dropna(), cats)\n",
    "polarcords_scores= cross_validation.cross_val_score(model, polarCordsOnly, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "print \"score with all additions:\",with_dummies_scores.mean()\n",
    "print \"with only new coordinate systems:\",without_xy_scores.mean()\n",
    "print \"scaled xy only score:\",without_newcords_scores.mean()\n",
    "print \"with only the xy coordinates:\", polarcords_scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score looks promising! Again, could be a little bit exaggerated of a score because of overfitting, but I'm interested to see what the kaggle score yields\n",
    "with all the bells and whistles:0.19575110795636985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data_dummies.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('bernoulinb.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is an improvement!we are now scoring 2.61102 and ranked at 673. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
