{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Iteration 3\n",
    "I want to implement a new method of XY coordinate cleaning here. Let's see if this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to keep using Dummies from iteration 2. Most of the changes to XY coords will be done in the cleanup function. I'm basing my changes off this script: https://www.kaggle.com/c/sf-crime/forums/t/18853/feature-engineering-of-lat-long-x-y-helps\n",
    "Basically, it uses feature engineering to make the coordinate use more intuitive. sklearn.preprocessing.StandardScaler scales the XY coords so that its variance and magnitude can't dwarf the other variables' features. With scaling, the XY coords are less likely to dominate the objective function. \n",
    "\n",
    "I also created new cartesian coordinate systems that are rotated around SF and a polar coordinate system. All of these center on the SF area and convey more relevant spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "def cleanupDummiesOld(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data =pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix=\"dow\")), axis=1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix=\"pds\")), axis=1)\n",
    "    return data\n",
    "oldData = pd.read_csv('train.csv')\n",
    "oldData = cleanupDummiesOld(oldData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Dates', u'Category', u'Descript', u'DayOfWeek', u'PdDistrict',\n",
       "       u'Resolution', u'Address', u'X', u'Y', u'rot45_X', u'rot45_Y',\n",
       "       u'rot30_X', u'rot30_Y', u'rot60_X', u'rot60_Y', u'radial_r', u'Year',\n",
       "       u'Month', u'Hour', u'dow_Friday', u'dow_Monday', u'dow_Saturday',\n",
       "       u'dow_Sunday', u'dow_Thursday', u'dow_Tuesday', u'dow_Wednesday',\n",
       "       u'pds_BAYVIEW', u'pds_CENTRAL', u'pds_INGLESIDE', u'pds_MISSION',\n",
       "       u'pds_NORTHERN', u'pds_PARK', u'pds_RICHMOND', u'pds_SOUTHERN',\n",
       "       u'pds_TARAVAL', u'pds_TENDERLOIN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_date(Dates):\n",
    "    \"\"\" Convert a date in YYYY-MM-DD HH:MM:SS to a tuple\n",
    "        containing year, month, day, and hours each expressed\n",
    "        as an integer. Used from Paul Ruvolo's example in bikeshare kaggle dataset\n",
    "    \"\"\"\n",
    "    return int(Dates[0:4]), int(Dates[5:7]), int(Dates[8:10]), int(Dates[11:13])\n",
    "\n",
    "def cleanupDummies(data):\n",
    "    data.X.replace(-120.5, data[\"X\"].median(), inplace = True)\n",
    "    data.Y.replace(90, data[\"Y\"].median(), inplace = True)\n",
    "    xy_scaler=preprocessing.StandardScaler()\n",
    "    xy_scaler.fit(data[[\"X\",\"Y\"]])\n",
    "    data[[\"X\",\"Y\"]]=xy_scaler.transform(data[[\"X\",\"Y\"]])\n",
    "    data[\"rot45_X\"]=0.707*data[\"Y\"]+0.707*data[\"X\"]\n",
    "    data[\"rot45_Y\"]=0.707*data[\"Y\"]-0.707*data[\"X\"]\n",
    "    data[\"rot30_X\"]=(1.732/2)*data[\"X\"]+(1./2)*data[\"Y\"]\n",
    "    data[\"rot30_Y\"]=(1.732/2)*data[\"Y\"]-(1./2)*data[\"X\"]\n",
    "    data[\"rot60_X\"]=(1./2)*data[\"X\"]+(1.732/2)*data[\"Y\"]\n",
    "    data[\"rot60_Y\"]=(1./2)*data[\"Y\"]-(1.732/2)*data[\"X\"]\n",
    "    data[\"radial_r\"]=np.sqrt(np.power(data[\"Y\"],2)+np.power(data[\"X\"],2))\n",
    "    \n",
    "    data[\"Year\"] = data.Dates.apply(lambda x: parse_date(x)[0])\n",
    "    data[\"Month\"] = data.Dates.apply(lambda x: parse_date(x)[1])\n",
    "    data[\"Hour\"] = data.Dates.apply(lambda x: parse_date(x)[3])\n",
    "    data =pd.concat((data, pd.get_dummies(data.DayOfWeek, prefix=\"dow\")), axis=1)\n",
    "    data = pd.concat((data, pd.get_dummies(data.PdDistrict, prefix=\"pds\")), axis=1)\n",
    "    return data\n",
    "\n",
    "data_dummies=pd.read_csv('train.csv')\n",
    "data_dummies= cleanupDummies(data_dummies)\n",
    "\n",
    "data_dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try these cross validation trials with dummies and without dummies to look at how much it may have helped our score. We'll also be uploading a test csv to kaggle. We're keeping with BernoulliNB for now because it predicts more accurately than the sample submission. Our next iteration will focus on algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original score: 0.220478593273\n",
      "score with all additions: 0.195751107956\n",
      "with only new coordinate systems: 0.197247620754\n",
      "scaled xy only score: 0.210547522574\n",
      "with only the xy coordinates: 0.220478593273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import cross_validation\n",
    "\n",
    "\n",
    "model = BernoulliNB()\n",
    "cats = data_dummies.Category.values\n",
    "origDrops = oldData.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "dummyDataDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "xyCordsDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"X\",\"Y\"], axis=1)\n",
    "\n",
    "newCordsDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"radial_r\"], axis=1)\n",
    "\n",
    "polarCordsOnly = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"X\", \"Y\"], axis=1)\n",
    "\n",
    "model.fit(origDrops.dropna(),cats)\n",
    "orig_score = cross_validation.cross_val_score(model, origDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "with_dummies_scores= cross_validation.cross_val_score(model, dummyDataDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(xyCordsDrops.dropna(), cats)\n",
    "without_xy_scores= cross_validation.cross_val_score(model, xyCordsDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(newCordsDrops.dropna(), cats)\n",
    "without_newcords_scores= cross_validation.cross_val_score(model, newCordsDrops, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "model.fit(polarCordsOnly.dropna(), cats)\n",
    "polarcords_scores= cross_validation.cross_val_score(model, polarCordsOnly, data_dummies[\"Category\"], cv=3)\n",
    "\n",
    "print \"original score:\", orig_score.mean()\n",
    "print \"score with all additions:\",with_dummies_scores.mean()\n",
    "print \"with only new coordinate systems:\",without_xy_scores.mean()\n",
    "print \"scaled xy only score:\",without_newcords_scores.mean()\n",
    "print \"with only the polar coordinates:\", polarcords_scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this training data, it looks like the best fit model is with all new coordinate system additions. Interestingly, scaling the XY coordinates on their own doesn't do anything for model accuracy. I'm submitting a test version with all new coordinate parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data_dummies.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(dummyDataDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('newcoords.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unfortunately, the test file generated with all new coordinates scores significantly lower on kaggle- a 2.9 to the current score of 2.6. I'm going to try the next best scoring combination of coordinate pairs. by dropping the XY coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data_dummies.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\", \"X\",\"Y\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(xyCordsDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('newcoords.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating use of the raw xy coords gets a 2.7 on kaggle, which is better than before, but still note better than Model 2. I'm going to try using just the scaled XY coordinates, which received the next best training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data_dummies.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\",\"Address\",\"Dates\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"radial_r\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(newCordsDrops.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('newcoords.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got a 2.638, whereas the model 2 score was 2.611. I'm trying just polar coordinates next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip, csv\n",
    "testDummies = pd.read_csv('test.csv')\n",
    "testDummies = cleanupDummies(testDummies)\n",
    "\n",
    "idx = testDummies.Id.values\n",
    "cats = data_dummies.Category.values\n",
    "\n",
    "droppedTestDummies = testDummies.drop([\"Id\", \"Address\", \"Dates\", \"DayOfWeek\", \"PdDistrict\", \"rot45_X\",\"rot45_Y\",\"rot30_X\",\"rot30_Y\",\"rot60_X\",\"rot60_Y\",\"X\", \"Y\"], axis=1)\n",
    "\n",
    "model = BernoulliNB()\n",
    "model.fit(polarCordsOnly.dropna(), cats)\n",
    "predicted = model.predict_proba(droppedTestDummies)\n",
    "\n",
    "labels =['Id']\n",
    "for i in model.classes_:\n",
    "    labels.append(i)\n",
    "with gzip.open('newcoords.csv.gz', 'wb') as outf:\n",
    "    fo =csv.writer(outf, lineterminator = '\\n' )\n",
    "    fo.writerow(labels)\n",
    "    \n",
    "    for i, pred in enumerate(predicted):\n",
    "        fo.writerow([i] + list(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's literally the same score as model 2. I'm wondering if there's a different algorithm I can use that will actually optimize the new coordinate structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forum entry I found discussing cleaning coordinates used random forests, so let's implement that next instead of Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21487575  0.20834985  0.20009293  0.21128195  0.16919176  0.09571713]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import cross_validation \n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "dummyDataDrops = data_dummies.drop([\"Address\",\"Category\",\"Dates\",\"Descript\",\"Resolution\", \"DayOfWeek\", \"PdDistrict\"], axis=1)\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, max_depth =18)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, dummyDataDrops, data_dummies[\"Category\"], cv=6)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks just as promising as the other Bernoulli-based models! I'll generate a test file with this. Hopefully the random-ness is not as overfit to the data as the previous attempts with Bernoulli and customized location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
