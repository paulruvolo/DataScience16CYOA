{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The lines of code that apply the models to the test data and generate the submission file are commented out.  This is because the files created are >500MB each, and we didn't want somebody to accidentally create a bunch of large files if they ran all the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import crime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(crime)\n",
    "train = crime.load_cleaned_train()\n",
    "test = crime.load_cleaned_test()\n",
    "\n",
    "# print train.info()\n",
    "# print test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is cleaned as described in `crime.py`.  In short, Year, Month, Day, Hour, and Minute columns are created, DayOfWeek, PdDistrict, and Category are encoded as integers, and invalid X and Y values are set to the median for that crime's PdDistrict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Split Train Data for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X','Y','Year','Hour','Minute','DoW','PdD','CornerCrime','ST_0','BogusReport','NBogusReport']\n",
    "X = train[predictors]\n",
    "y = train.CategoryNumber\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `stratify` parameter of `train_test_split` requires scikit-learn-0.17, but ensures that the proportion of categories is maintained in the split.  The biggest thing that this does is make it so that we always get at least one crime from each category in the training set.  Our models can only predict based on what they have seen before, so it is crucial that we train them with all possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(alg, X_train, X_test, y_train, y_test):\n",
    "    predictor_sets = (\n",
    "#         ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD', 'Hour', 'DoW'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD', 'Hour'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'PdD', 'Hour', 'DoW'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'Hour', 'DoW'],\n",
    "#         ['X', 'Y', 'CornerCrime'],\n",
    "#         ['X', 'Y', 'CornerCrime','BogusReport'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'PdD', 'Hour'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'PdD', 'BogusReport'],\n",
    "#         ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime'],\n",
    "        ['X', 'Y', 'DoW', 'Year', 'CornerCrime','BogusReport','PdD','ST_0','Minute'],\n",
    "        ['X', 'Y', 'DoW', 'Year', 'CornerCrime','BogusReport','PdD','ST_0','Minute','NBogusReport'],\n",
    "        ['X', 'Y', 'DoW', 'Year', 'CornerCrime','BogusReport','PdD','ST_0','Minute','NBogusReport','Hour'],\n",
    "#         ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD','Afternoon','Night','Morning','Evening'],\n",
    "#         ['DoW', 'Hour', 'Year', 'CornerCrime', 'ST_0', 'ST_1'],\n",
    "#         ['X', 'Y', 'Hour', 'CornerCrime', 'ST_0','PdD_0','PdD_1','PdD_2','PdD_3','PdD_4','PdD_5','PdD_6','PdD_7','PdD_8','PdD_9'],\n",
    "#         ['X', 'Y', 'Hour', 'DoW', 'CornerCrime'],\n",
    "#         ['X', 'Y', 'ST_0', 'CornerCrime','PdD_0','PdD_1','PdD_2','PdD_3','PdD_4','PdD_5','PdD_6','PdD_7','PdD_8','PdD_9'],\n",
    "#         ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime','PdD_0','PdD_1','PdD_2','PdD_3','PdD_4','PdD_5','PdD_6','PdD_7','PdD_8','PdD_9','Afternoon','Night','Morning','Evening']\n",
    "    )\n",
    "\n",
    "    for predictors in predictor_sets:\n",
    "        alg.fit(X_train[predictors], y_train)\n",
    "        p = alg.predict_proba(X_test[predictors])\n",
    "        print crime.logloss(y_test, p), predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have something to compare to, we've created a baseline model that guesses based on the crime rates in each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class baseline(object):\n",
    "    def __init__(self):\n",
    "        self.has_fit = False\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = X_train.copy()\n",
    "        X_train['CategoryNumber'] = y_train\n",
    "        groups = X_train.groupby(['PdD', 'CategoryNumber'])\n",
    "\n",
    "        # Tally up the counts of each Category in each PdDistrict\n",
    "        num_districts = len(X_train.PdD.unique())\n",
    "        num_categories = len(y_train.unique())\n",
    "        self.district_rates = np.zeros((num_districts, num_categories))\n",
    "        for ind,data in groups:\n",
    "            self.district_rates[ind] = len(data)\n",
    "\n",
    "        # Normalize values\n",
    "        self.district_rates /= self.district_rates.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.has_fit = True\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        if self.has_fit:\n",
    "            predictions = X_test.PdD.apply(lambda x: self.district_rates[x,:])\n",
    "            return pd.DataFrame(predictions.tolist()).values  # to get a numpy array of the correct shape\n",
    "        return None\n",
    "\n",
    "alg = baseline()\n",
    "predictors = ['PdD']\n",
    "alg.fit(X_train[predictors], y_train)\n",
    "p = alg.predict_proba(X_test[predictors])\n",
    "print crime.logloss(y_test, p)\n",
    "\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'baseline_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.61645 on the test data, a very similar score to the cross validation.  This isn't too surprising since the way the train and test data are split up are by every other week, so our cross validation train-test split is pretty representative of the data as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we've chosen to try is the k-Nearest Neighbors model, partially for the fact that you can quite literally look at which crimes occurred near each other using the X and Y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.11685209372 ['X', 'Y', 'Hour', 'PdD', 'CornerCrime', 'ST_0']\n",
      "5.2368918436 ['X', 'Y', 'PdD', 'Hour', 'ST_0', 'ST_1']\n"
     ]
    }
   ],
   "source": [
    "alg = KNeighborsClassifier(n_neighbors=50)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model did the best when it only used the spatial data, the X and Y columns.  It is also performing worse than our baseline model, but let's see how it does with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y']\n",
    "alg = KNeighborsClassifier(n_neighbors=50)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'k-nn_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 5.32130 on the test data, which is a little worse than the cross validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we decided to see how our old friend the Logistic Regression would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56665986522 ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "2.54508352077 ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9', 'Afternoon', 'Night', 'Morning', 'Evening']\n",
      "2.56204104465 ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "2.54155475454 ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9', 'Afternoon', 'Night', 'Morning', 'Evening']\n"
     ]
    }
   ],
   "source": [
    "alg = LogisticRegression()\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model seems to do slightly better when it's given all of the predictors, but is still not as good as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = LogisticRegression()\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'lr_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.65839 on the test data, quite close to the cross validation score but still not as good as the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58949857035 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n",
      "2.59203895398 ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime']\n"
     ]
    }
   ],
   "source": [
    "alg = tree.DecisionTreeClassifier(max_depth=3)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try giving the model all the predictors since its scores are so close for just using two and using all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = tree.DecisionTreeClassifier(max_depth=3, min_samples_leaf=5)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'dt_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.62696 on the test data.  Same story here:  very close to the cross validation score, but worse than our baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67518828608 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n"
     ]
    }
   ],
   "source": [
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'Year', 'Month', 'Hour', 'DoW', 'PdD']\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'gb_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.69673 on the test data.  Again, no surprises here. Additionally, this model takes a very long time computationally to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.37321471025 ['X', 'Y', 'DoW', 'Year', 'CornerCrime', 'BogusReport', 'PdD', 'ST_0', 'Minute']\n",
      "2.3694449714 ['X', 'Y', 'DoW', 'Year', 'CornerCrime', 'BogusReport', 'PdD', 'ST_0', 'Minute', 'NBogusReport']\n",
      "2.35486923645 ['X', 'Y', 'DoW', 'Year', 'CornerCrime', 'BogusReport', 'PdD', 'ST_0', 'Minute', 'NBogusReport', 'Hour']\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(n_estimators=25, max_depth=15)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more trees (`n_estimators`) seems to improve things, but makes a senior laptop with 8GB of RAM a bit sad.  If your computer can handle it, try increasing this parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y']\n",
    "alg = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'rf_xy_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.45770 on the test data, better than the baseline!  It's interesting how well this model performed with so few predictors being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'DoW', 'Hour', 'Year']\n",
    "alg = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'rf_more_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.46047 on the test data, slightly worse than the Random Forest using only the X and Y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'PdD', 'Hour']\n",
    "alg = RandomForestClassifier(n_estimators=25, max_depth=10)\n",
    "# crime.create_submission(alg, X, y, test, predictors, 'rf_axph_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only the X,Y,PdD, and Hour predictors the score can be improved to 2.44754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'DoW', 'Hour', 'Year']\n",
    "alg = RandomForestClassifier(n_estimators=25, max_depth=15)\n",
    "crime.create_submission(alg, X, y, test, predictors, 'rf_xydhy_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scored a 2.43554 on Kaggle, but we're starting to get to the point in which with better computers, we could perform better. This is fine, but now awesome for our learning. \n",
    "\n",
    "Adding the corner crime indicator gave an impressive score of 2.41737."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'DoW', 'Year', 'CornerCrime', 'BogusReport', 'PdD', 'ST_0', 'Minute']\n",
    "alg = RandomForestClassifier(n_estimators=20, max_depth=10)\n",
    "crime.create_submission(alg, X, y, test, predictors, 'v1_rfc_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got a score of 2.38158."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['X', 'Y', 'DoW', 'Year', 'CornerCrime', 'BogusReport', 'PdD', 'ST_0', 'Minute', 'NBogusReport', 'Hour']\n",
    "alg = RandomForestClassifier(n_estimators=25, max_depth=15)\n",
    "crime.create_submission(alg, X, y, test, predictors, 'v3_rfc_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got a score of 2.33845."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.62482822734 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n",
      "2.62766723051 ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime']\n",
      "2.60877973281 ['X', 'Y', 'CornerCrime', 'ST_0', 'PdD', 'Afternoon', 'Night', 'Morning', 'Evening']\n",
      "2.56666551916 ['X', 'Y', 'Hour', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "2.55127520511 ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9', 'Afternoon', 'Night', 'Morning', 'Evening']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "alg = BernoulliNB(fit_prior=True, binarize=0.0, alpha=0.25)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58774824065 ['X', 'Y', 'CornerCrime']\n",
      "3.58199885915 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n",
      "3.57840619939 ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime']\n",
      "3.58100998049 ['X', 'Y', 'Hour', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "3.58784805018 ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "alg = AdaBoostClassifier()\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.12364876848 ['X', 'Y', 'CornerCrime']\n",
      "12.6827841013 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n",
      "11.9044417276 ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime']\n",
      "12.6980398955 ['X', 'Y', 'Hour', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "7.15624551676 ['X', 'Y', 'ST_0', 'ST_1', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "alg = BaggingClassifier(n_estimators=20)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.61062486967 ['X', 'Y', 'CornerCrime']\n",
      "15.4302319074 ['X', 'Y', 'CornerCrime', 'PdD', 'Hour']\n",
      "14.1251773967 ['X', 'Y', 'DoW', 'Hour', 'Year', 'CornerCrime']\n",
      "15.4325900837 ['X', 'Y', 'Hour', 'CornerCrime', 'ST_0', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n",
      "7.648138786 ['X', 'Y', 'ST_0', 'CornerCrime', 'PdD_0', 'PdD_1', 'PdD_2', 'PdD_3', 'PdD_4', 'PdD_5', 'PdD_6', 'PdD_7', 'PdD_8', 'PdD_9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "alg = ExtraTreesClassifier(n_estimators = 20)\n",
    "cross_validate(alg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telling a Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this modelling exploration, we find that the methods which call out the categorical nature of the data - logistic regression, decision trees, and random forest classifiers - perform highly. Particularly relevant is that the predictors of location and time of day may be particularly telling, followed by year and day of week. In our exploration phase, these relationships were also evident. What we're seeing, at a high level, is that crime reporting or crime report filing over the past 10+ years in San Francisco has followed a similar pattern - the same types of crime are being committed with relatively similar frequency, in the same general locations, and are being reported by the expected reporting entity. \n",
    "\n",
    "What may be interesting to explore will be whether the type of crime that it is, different slices of time of year, or connecting reporting time and type of crime may yield more exacting results when predicting crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is such a simplistic, hard-coded model performing better than most of these scikit-learn models?  What is it about this dataset that makes these models perform as they do?  Would it be worth tweaking the parameters of one of these models to try and improve the score, or are they just the wrong models to be using for this problem?  How can we answer these questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
